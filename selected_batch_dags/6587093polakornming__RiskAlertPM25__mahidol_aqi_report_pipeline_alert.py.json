{
  "unique_key": "6587093polakornming/RiskAlertPM25:scripts/mahidol_aqi_report_pipeline_alert.py",
  "repo": "6587093polakornming/RiskAlertPM25",
  "repo_url": "https://github.com/6587093polakornming/RiskAlertPM25",
  "stars": 0,
  "license": "MIT",
  "file_path": "scripts/mahidol_aqi_report_pipeline_alert.py",
  "detected_airflow_version": "2.x",
  "sampled_versions": [
    {
      "version": "latest",
      "commit": "36617653662e858934807692fe2074af556a89ff",
      "code": "import json\nimport requests\nimport logging\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom bs4 import BeautifulSoup\nimport smtplib\nfrom email.message import EmailMessage\nimport configparser\n\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.exceptions import AirflowSkipException\n\n# ============ CONFIG ============ #\nBASE_DIR = Path(__file__).resolve().parent.parent\nDATA_DIR = BASE_DIR / \"data\"\nHTML_FILE = DATA_DIR / \"mahidol_aqi.html\"\nOUTPUT_FILE = DATA_DIR / \"tmp_mahidol.json\"\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\nLATITUDE = 13.794606\nLONGITUDE = 100.327256\nDESCRIPTION = \"‡∏Ñ‡∏ì‡∏∞‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏°‡∏´‡∏¥‡∏î‡∏•\"\nCOUNTRY = \"Thailand\"\nSTATE = \"Nakhon Pathom\"\nCITY = \"Salaya\"\n\n# ============ FUNCTIONS ============ #\ndef get_cursor():\n    hook = PostgresHook(postgres_conn_id=\"postgres_conn\")\n    conn = hook.get_conn()\n    return conn, conn.cursor()\n\ndef is_data_in_db(date_time_str):\n    conn, cursor = get_cursor()\n    try:\n        cursor.execute(\"\"\"\n            SELECT 1 FROM factmahidolaqitable\n            WHERE date_time = %s\n            LIMIT 1\n        \"\"\", (date_time_str,))\n        return cursor.fetchone() is not None\n    finally:\n        cursor.close()\n        conn.close()\n\n# --- ====== TASK 1 ====== ---\ndef get_data_mahidol_aqi_report():\n    URL = \"https://mahidol.ac.th/aqireport/\"\n    try:\n        res = requests.get(URL, timeout=10)\n        res.raise_for_status()\n        tmp_path = Path(str(HTML_FILE) + \".tmp\")\n        with open(tmp_path, 'w', encoding='utf-8') as f:\n            f.write(res.text)\n        tmp_path.rename(HTML_FILE)\n        logging.info(\"Saved Mahidol AQI HTML successfully.\")\n\n    except Exception as e:\n        logging.error(f\"Failed to fetch Mahidol AQI report: {e}\")\n        raise\n\ndef get_clean_text(element_id, soup: BeautifulSoup):\n    element = soup.find(id=element_id)\n    if element:\n        return element.get_text(strip=True, separator=\" \").split()[0].strip()\n    return None\n\ndef convert_to_datetime(datetime_str):\n    if datetime_str:\n        try:\n            return datetime.strptime(datetime_str, \"%d %B %Y, %H:%M hrs.\")\n        except ValueError as e:\n            logging.error(f\"Error parsing datetime: {e}\")\n    return None\n\ndef get_main_pollution_text(soup: BeautifulSoup):\n    try:\n        aqi_container = soup.find(\"div\", class_=\"fa-10x\")\n        if aqi_container:\n            h4_element = aqi_container.find(\"h4\")\n            if h4_element:\n                return h4_element.get_text(strip=True).replace(\"(\", \"\").replace(\")\", \"\").strip()\n    except Exception as e:\n        logging.error(f\"Error in get_main_pollution_text: {e}\")\n    return None\n\n# --- ====== TASK 2 ====== ---\ndef create_json_object():\n    try:\n        with open(HTML_FILE, \"r\", encoding=\"utf-8\") as file:\n            soup = BeautifulSoup(file, \"html.parser\")\n    except Exception as e:\n        logging.error(f\"Error reading HTML file: {e}\")\n        raise\n\n    try:\n        date_en = soup.find(id=\"ContentPlaceHolder1_lblDateTimeEN\")\n        datetime_value = f\" {date_en.get_text(strip=True)}\".strip() if date_en else None\n        datetime_value_obj = convert_to_datetime(datetime_value)\n\n        air_quality_div = soup.find(\"div\", class_=\"alert\")\n        air_quality_text = air_quality_div.find(\"h4\").get_text(strip=True) if air_quality_div else None\n        main_pollution = get_main_pollution_text(soup)\n\n        data = {\n            \"Datetime\": datetime_value_obj.isoformat() if datetime_value_obj else None,\n            \"Air Quality\": air_quality_text,\n            \"Main Pollution\": main_pollution,\n            \"AQI\": get_clean_text(\"ContentPlaceHolder1_lblAQI\", soup),\n            \"PM25\": get_clean_text(\"ContentPlaceHolder1_lblHourlyPM25\", soup),\n            \"PM10\": get_clean_text(\"ContentPlaceHolder1_lblHourlyPM10\", soup),\n            \"O3\": get_clean_text(\"ContentPlaceHolder1_lblHourlyO3\", soup),\n            \"CO\": get_clean_text(\"ContentPlaceHolder1_lblHourlyCO\", soup),\n            \"NO2\": get_clean_text(\"ContentPlaceHolder1_lblHourlyNO2\", soup),\n            \"SO2\": get_clean_text(\"ContentPlaceHolder1_lblHourlySO2\", soup),\n            \"Temperature\": get_clean_text(\"ContentPlaceHolder1_lblTemperature\", soup),\n            \"Humidity\": get_clean_text(\"ContentPlaceHolder1_lblHumidity\", soup),\n            \"Wind Speed\": get_clean_text(\"ContentPlaceHolder1_lblWindSpeed\", soup),\n            \"Wind Direction\": get_clean_text(\"ContentPlaceHolder1_lblWindDirection\", soup),\n            \"Rainfall\": get_clean_text(\"ContentPlaceHolder1_lblRainfall\", soup),\n            \"Solar Radiation\": get_clean_text(\"ContentPlaceHolder1_lblSolar\", soup),\n        }\n        logging.info(\"Extracted data: %s\", data)\n\n        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö datetime ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î\n        iso_string = data[\"Datetime\"] #FORMAT eg. \"2025-04-02T21:00:00\"\n        if iso_string:\n            dt_obj = datetime.strptime(iso_string, \"%Y-%m-%dT%H:%M:%S\")\n            current_dt_str = dt_obj.strftime(\"%Y-%m-%d %H:%M\")\n        else:\n            raise Exception(f\"Data DateTime has Error value of Error: {current_dt_str}\")\n        \n        # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Database ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ datetime ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n        data_time_now_obj = datetime.now()+ timedelta(hours=7)\n        data_time_now_str = data_time_now_obj.strftime(\"%Y-%m-%d %H:00\") \n        if is_data_in_db(data_time_now_str):\n            raise AirflowSkipException(f\"Data for {data_time_now_str} already exists in DB. Skipping DAG.\")\n\n        # ‡πÇ‡∏´‡∏•‡∏î datetime ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á\n        if OUTPUT_FILE.exists():\n            with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f_check:\n                prev_data = json.load(f_check)\n            prev_dt_str = prev_data[\"Datetime\"]\n            prev_dt_obj = datetime.strptime(prev_dt_str, \"%Y-%m-%dT%H:%M:%S\")\n            prev_dt_str = prev_dt_obj.strftime(\"%Y-%m-%d %H:%M\")\n            if current_dt_str == prev_dt_str and is_data_in_db(prev_dt_str):\n                raise Exception(f\"Data has not been updated yet: {current_dt_str} == {prev_dt_str}\")\n\n        # ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ \"Atomic File Write\":\n        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (.tmp) ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢ rename ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á\n        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÉ‡∏ô Task ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n        tmp_path = Path(str(OUTPUT_FILE) + \".tmp\")\n        with open(tmp_path, 'w', encoding='utf-8') as f:\n            json.dump(data, f, ensure_ascii=False, indent=2)\n\n        tmp_path.rename(OUTPUT_FILE)\n        logging.info(\"Response saved successfully.\")\n\n    except Exception as e:\n        logging.error(f\"Error create_json_object: {e}\")\n        raise\n\n# --- ====== TASK 3 ====== ---\ndef load_mahidol_aqi_to_postgres():\n    logging.info(\"Starting load_mahidolAQI_to_postgres\")\n    conn, cursor = get_cursor()\n    try:\n        # Load pollution mapping from config\n        with open('/opt/airflow/config/mapping_main_pollution.json', 'r', encoding='utf-8') as mf:\n            pollution_mapping = json.load(mf)\n\n        # Load raw JSON data\n        with open('/opt/airflow/data/tmp_mahidol.json', 'r', encoding='utf-8') as f:\n            raw_data: dict = json.load(f)\n\n        date_time_str = raw_data[\"Datetime\"]\n        date_time_obj = datetime.strptime(date_time_str, \"%Y-%m-%dT%H:%M:%S\")\n        date_str = date_time_obj.strftime(\"%Y-%m-%d\")\n        time_str = date_time_obj.strftime(\"%H:%M:%S\")\n\n        # --- Step 1: Insert into dimDateTimeTable (if not exists) ---\n        cursor.execute(\"\"\"\n            INSERT INTO dimDateTimeTable (date_time, date, time, day, month, year, hour)\n            VALUES (%s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT (date_time) DO NOTHING;\n        \"\"\", (\n            date_time_obj,\n            date_str,\n            time_str,\n            date_time_obj.day,\n            date_time_obj.month,\n            date_time_obj.year,\n            date_time_obj.hour\n        ))\n\n        # --- Step 2: Insert into dimLocationTable (if not exists) ---\n        cursor.execute(\"\"\"\n            SELECT description, location_id FROM dimLocationTable\n            WHERE description = %s;\n        \"\"\", (DESCRIPTION,))\n        result = cursor.fetchone()\n        print(f\"fecth cursor result find location id: {result}\")\n        if result:\n            location_id = result[1]\n        else:\n            cursor.execute(\"\"\"\n                INSERT INTO dimLocationTable (latitude, longitude, description, country, state, city)\n                VALUES (%s, %s, %s, %s, %s, %s)\n                RETURNING location_id;\n            \"\"\", (LATITUDE, LONGITUDE, DESCRIPTION, COUNTRY, STATE, CITY))\n            location_id = cursor.fetchone()[0]\n\n        # --- Step 3: Insert into dimMainPollutionTable (if not exists) ---\n        main_code = raw_data[\"Main Pollution\"]\n        if main_code not in pollution_mapping:\n            logging.warning(f\"Pollution code '{main_code}' not found in mapping file.\")\n        mapping = pollution_mapping.get(main_code, {\"unit\": \"unknown\", \"name_pollution\": main_code})\n        cursor.execute(\"\"\"\n            INSERT INTO dimMainPollutionTable (main_pollution_code, unit, name_pollution)\n            VALUES (%s, %s, %s)\n            ON CONFLICT (main_pollution_code) DO NOTHING;\n        \"\"\", (main_code, mapping['unit'], mapping['name_pollution']))\n\n        # --- Step 4: Insert into factMahidolAqiTable ---\n        def clean_value(val):\n            try:\n                return round(float(val), 2)\n            except:\n                return None\n\n        cursor.execute(\"\"\"\n            INSERT INTO factmahidolaqitable (\n                date_time, location_id, main_pollution_code,\n                air_quality_text, aqi,\n                pm25_value, pm10_value, o3_value,\n                co_value, no2_value, so2_value,\n                temperature_c, humidity, wind_speed,\n                wind_direction, rainfall, solar_radiation\n            )\n            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT (date_time, location_id, main_pollution_code) DO NOTHING;\n        \"\"\", (\n            date_time_obj,\n            location_id,\n            main_code,\n            raw_data.get(\"Air Quality\"),\n            clean_value(raw_data.get(\"AQI\")),\n            clean_value(raw_data.get(\"PM25\")),\n            clean_value(raw_data.get(\"PM10\")),\n            clean_value(raw_data.get(\"O3\")),\n            clean_value(raw_data.get(\"CO\")),\n            clean_value(raw_data.get(\"NO2\")),\n            clean_value(raw_data.get(\"SO2\")),\n            clean_value(raw_data.get(\"Temperature\")),\n            clean_value(raw_data.get(\"Humidity\")),\n            clean_value(raw_data.get(\"Wind Speed\")),\n            clean_value(raw_data.get(\"Wind Direction\")),\n            clean_value(raw_data.get(\"Rainfall\")),\n            clean_value(raw_data.get(\"Solar Radiation\"))\n        ))\n        conn.commit()\n        logging.info(\"Finished load_mahidolAQI_to_postgres\")\n\n    except Exception as e:\n        conn.rollback()\n        logging.error(f\"Error during ETL: {e}\")\n        raise\n\n    finally:\n        cursor.close()\n        conn.close()\n        logging.info(\"ETL to PostgreSQL Completed\")\n\n# --- ====== TASK 4 ====== ---\ndef alert_email():\n    def email_alert(subject: str, body: str, to: str):\n        msg = EmailMessage()\n        msg.set_content(body)\n        msg[\"Subject\"] = subject\n        msg[\"To\"] = to\n\n        user = \"supakorn.ming@gmail.com\"  # Sender Email\n        msg[\"From\"] = user\n\n        config_path = BASE_DIR / \"config\" / \"config.conf\"\n        config = configparser.ConfigParser()\n        config.read(config_path)\n        password = config.get(\"email\", \"password\")\n        logging.info(f\"Sending email from {user} to {to} using config {config_path}\")\n\n        try:\n            with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n                server.starttls()\n                server.login(user, password)\n                server.send_message(msg)\n                logging.info(f\"Email sent to {to}\")\n        except Exception as e:\n            logging.error(f\"Failed to send email to {to}: {e}\")\n            raise\n\n    def clean_value(val):\n        try:\n            return round(float(val), 2)\n        except (TypeError, ValueError):\n            return None\n\n    try:\n        with open(DATA_DIR / \"tmp_mahidol.json\", 'r', encoding='utf-8') as f:\n            raw_data = json.load(f)\n    except Exception as e:\n        logging.error(f\"Error loading AQI JSON data: {e}\")\n        raise\n\n    aqi_value = clean_value(raw_data.get(\"AQI\"))\n    body = None\n\n    if aqi_value is not None:\n        if 0 <= aqi_value <= 50:\n            logging.info(f\"AQI is safe ({aqi_value}). No alert needed.\")\n            raise AirflowSkipException(\"AQI in safe range. Skipping alert.\")\n        elif 51 <= aqi_value <= 100:\n            logging.info(f\"AQI is ({aqi_value}) alert. 51 <= aqi_value <= 100\")\n            body = (\n                f\"‡∏Ñ‡πà‡∏≤ AQI ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {aqi_value}\\n\"\n                \"‚ö†Ô∏è AQI ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (51-100)\\n\"\n                \"‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏•‡∏≤‡∏á‡πÅ‡∏à‡πâ‡∏á‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥\\n\"\n                \"‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏£‡∏Ñ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß‡∏Ñ‡∏ß‡∏£‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡πÑ‡∏≠ ‡∏´‡∏≤‡∏¢‡πÉ‡∏à‡∏•‡∏≥‡∏ö‡∏≤‡∏Å ‡∏£‡∏∞‡∏Ñ‡∏≤‡∏¢‡πÄ‡∏Ñ‡∏∑‡∏≠‡∏á‡∏ï‡∏≤\"\n            )\n        elif 101 <= aqi_value <= 200:\n            logging.info(f\"AQI is ({aqi_value}) alert. 101 <= aqi_value <= 200\")\n            body = (\n                f\"‡∏Ñ‡πà‡∏≤ AQI ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {aqi_value}\\n\"\n                \"‚ö†Ô∏è AQI ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û (101-200)\\n\"\n                \"‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ñ‡∏ß‡∏£‡∏•‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏•‡∏≤‡∏á‡πÅ‡∏à‡πâ‡∏á ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏£‡∏´‡∏¢‡∏∏‡∏î‡∏û‡∏±‡∏Å\\n\"\n                \"‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏£‡∏Ñ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏Å ‡πÅ‡∏•‡∏∞‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥\"\n            )\n        elif aqi_value > 200:\n            logging.info(f\"AQI is ({aqi_value}) alert. aqi_value > 200\")\n            body = (\n                f\"‡∏Ñ‡πà‡∏≤ AQI ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô {aqi_value}\\n\"\n                \"üö® AQI ‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏Å (‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 200)\\n\"\n                \"‡∏Ñ‡∏ß‡∏£‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏•‡∏≤‡∏á‡πÅ‡∏à‡πâ‡∏á‡∏ó‡∏∏‡∏Å‡∏ä‡∏ô‡∏¥‡∏î ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ï‡∏ô‡πÄ‡∏≠‡∏á\\n\"\n                \"‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏£‡∏£‡∏µ‡∏ö‡∏û‡∏ö‡πÅ‡∏û‡∏ó‡∏¢‡πå‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\"\n            )\n        else:\n            logging.warning(f\"AQI has unexpected value: {aqi_value}\")\n    else:\n        logging.warning(\"AQI value not found in data.\")\n        raise AirflowSkipException(\"No AQI value to evaluate.\")\n\n    users_file = BASE_DIR / \"config\" / \"pm25_alert_emails.txt\"\n    try:\n        with open(users_file, \"r\", encoding=\"utf-8\") as file:\n            recipients = [line.strip() for line in file if line.strip()]\n    except Exception as e:\n        logging.error(f\"Could not load email list: {e}\")\n        raise\n\n    if not recipients:\n        logging.warning(\"No recipients found in email list. Skipping alert.\")\n        raise AirflowSkipException(\"No recipients to send to.\")\n\n    for email in recipients:\n        email_alert(subject=\"PM2.5 Alert Notification\", body=body, to=email)\n\n    logging.info(\"All alert emails sent successfully.\")\n\n# ============ DAG ============ #\ndefault_args = {\n    'owner': 'Polakorn Anantapakorn Ming',\n    'start_date': datetime(2025, 3, 20),\n}\n\nwith DAG(\n    dag_id='mahidol_aqi_pipeline_v2_alert_1',\n    # schedule_interval='30 * * * *',\n    schedule_interval=None,\n    default_args=default_args,\n    description='A simple data pipeline for Mahidol AQI report',\n    catchup=False,\n) as dag:\n\n    # t1 = PythonOperator(\n    #     task_id='scraping_mahidol_aqi_report',\n    #     python_callable=get_data_mahidol_aqi_report\n    # )\n\n    # t2 = PythonOperator(\n    #     task_id='create_json_mahidol_aqi',\n    #     python_callable=create_json_object\n    # )\n\n    # t3 = PythonOperator(\n    #     task_id='load_data_mahidolAQI_to_postgresql',\n    #     python_callable=load_mahidol_aqi_to_postgres\n    # )\n\n    t4 = PythonOperator(\n        task_id='alert_email',\n        python_callable=alert_email\n    )\n\n    # t1 >> t2 >> [t3, t4]\n    t4\n"
    }
  ],
  "sampled_time": "2025-08-22T20:56:54.425496Z",
  "analysis": {
    "is_production_dag": true,
    "is_airflow_2": true,
    "processing_type": "batch",
    "topology_pattern": "linear",
    "description": "ETL pipeline that scrapes Mahidol University AQI data from website, processes it, loads to PostgreSQL database, and sends email alerts",
    "complexity_score": 6
  }
}