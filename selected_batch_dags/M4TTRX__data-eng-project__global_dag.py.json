{
  "unique_key": "M4TTRX/data-eng-project:dags/global_dag.py",
  "repo": "M4TTRX/data-eng-project",
  "repo_url": "https://github.com/M4TTRX/data-eng-project",
  "stars": 2,
  "license": "Apache-2.0",
  "file_path": "dags/global_dag.py",
  "detected_airflow_version": "2.x",
  "sampled_versions": [
    {
      "version": "latest",
      "commit": "670f3c7fe93cd230a70fd98d6360ab1450c44970",
      "code": "from cmath import nan\nimport airflow\nimport datetime\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators.python_operator import PythonOperator, BranchPythonOperator\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.utils.task_group import TaskGroup\nimport pandas as pd\npd.options.mode.chained_assignment = None  # default='warn'\n\nDEATH_DATASET_ID = '5de8f397634f4164071119c5'\nTHERMAL_DATASET_ID = '63587afb1cc488641390f68e'\nNUCLEAR_DATASET_ID = '63587afc1e8e90e9ce487174'\nCITY_GEO_DATA_ID = 'dbe8a621-a9c4-4bc3-9cae-be1699c5ff25'\nINGESTION_DATA_PATH = 'dags/data/ingestion/'\nDATA_GOUV_BASE_URL = 'https://www.data.gouv.fr/api/1/datasets/'\nGET_DEATH_DATASET_URL = DATA_GOUV_BASE_URL + DEATH_DATASET_ID\nGET_THERMAL_DATASET_URL = 'https://www.data.gouv.fr/api/1/datasets/63587afb1cc488641390f68e/'\nGET_NUCLEAR_DATAET_URL = 'https://www.data.gouv.fr/api/1/datasets/63587afc1e8e90e9ce487174/'\nCITY_GEO_DATASET_URL = 'https://static.data.gouv.fr/resources/communes-de-france-base-des-codes-postaux/20200309-131459/communes-departement-region.csv'\nSTAGING_DATA_PATH = 'dags/data/staging/'\n\nDEATH_INSERTION_QUERIES = 'death_insert.sql'\nPLANT_INSERTION_QUERIES = 'plant_insert.sql'\nDEATH_INSERTION_QUERIES_PATH = f'dags/sql/tmp/{DEATH_INSERTION_QUERIES}'\nPLANT_INSERTION_QUERIES_PATH = f'dags/sql/tmp/{PLANT_INSERTION_QUERIES}'\n\n\n# DAG definition\n\ndefault_args_dict = {\n    'start_date': airflow.utils.dates.days_ago(0),\n    'concurrency': 1,\n    'schedule_interval': None,\n    'retries': 1,\n    'retry_delay': datetime.timedelta(seconds=10),\n}\n\nglobal_dag = DAG(\n    dag_id='global_dag',\n    default_args=default_args_dict,\n    catchup=False,\n    template_searchpath=['/opt/airflow/dags/']\n)\n\n# Python functions\n# ===================\ndef _import_thermal_clean_data():\n    data_1 = pd.read_csv(\n        './dags/data/ingestion/thermal_plants_.csv', error_bad_lines=False, sep=';')\n    data_1 = data_1.drop(columns={'perimetre_spatial', 'filiere', 'combustible',\n                         'reserve_secondaire_maximale', 'sous_filiere', 'unite'})\n    data_1 = data_1.rename(columns={'centrale': 'plant', 'point_gps_wsg84': 'position', 'commune': 'city',\n                           'date_de_mise_en_service_industrielle': 'start_date', 'puissance_installee': 'power (MW)'})\n    data_1.to_csv('./dags/data/staging/thermal_plants_clean.csv')\n\n\ndef _import_nuclear_clean_data():\n    data_1 = pd.read_csv('./dags/data/ingestion/nuclear.csv',\n                         error_bad_lines=False, sep=';')\n    data_1 = data_1.drop(columns={'reserve_secondaire_maximale', 'puissance_minimum_de_conception',\n                         'sub_sector', 'perimetre_spatial', 'combustible', 'filiere', 'unite'})\n    data_1 = data_1.rename(columns={'centrale': 'plant', 'sous_filiere': 'sub_sector', 'contrat_programme': 'contract', 'point_gps_wsg84': 'position',\n                           'commune': 'city', 'date_de_mise_en_service_industrielle': 'start_date', 'puissance_installee': 'power (MW)'})\n    data_1.to_csv('./dags/data/staging/nuclear_clean_datas.csv')\n\n\ndef get_redis_client():\n    import redis\n    return redis.Redis(host='redis', port=6379, db=0)\n\n\ndef _load_data_from_ingestion():\n    import os\n    death_files = [os.path.join(root, name)\n                   for root, dirs, files in os.walk(INGESTION_DATA_PATH)\n                   for name in files\n                   if name.startswith((\"death_\"))]\n\n    # pull imported files from redis\n    r = get_redis_client()\n\n    imported_deaths = [file_path.decode(\n        \"utf-8\") for file_path in r.lrange('imported_death_files', 0, -1)]\n\n    # load the ones that have not been loaded yet\n    files_to_load = [\n        file_path for file_path in death_files if file_path not in imported_deaths]\n    print(f'{len(imported_deaths)}/{len(death_files)} files already imported. Importing {len(files_to_load)} files: {str(files_to_load)}')\n    import json\n    import hashlib\n    for file_path in files_to_load:\n        file = open(file_path, 'r')\n        for line in file.readlines():\n            # hash the person's name, we dont need it and it increases privacy\n            dead_person = {\n                'id': hashlib.sha1(line[:80].encode()).hexdigest(),\n                'location': line[162:167].strip(),\n                'death_date': line[154:162].strip(),\n                'birth_date': line[81:89].strip()\n            }\n            r.lpush('death_raw', json.dumps(dead_person))\n\n        r.lpush('imported_death_files', file_path)\n        print(f'{file} successfully imported')\n    # preliminary processing and store\n    return\n\ndef to_postgres_date(raw_date : str):\n    try:\n        return str(datetime.datetime.strptime(raw_date, '%Y%m%d'))[:10]\n    except:\n        try:\n            return str(datetime.datetime.strptime(raw_date, '%Y-%m-%d'))[:10]\n        except:\n            return None\n\ndef _cleanse_death_data():\n    import json\n    r = get_redis_client()\n    # load imports from redis\n    result = r.lrange('death_raw', 0, -1)\n    death_data = [json.loads(element.decode(\"utf-8\"))\n            for element in result]\n    import pandas as pd\n    print(\"Successfully loaded death data\")\n    insee_code_to_geo = {loc['code_commune_INSEE']: (loc['latitude'], loc['longitude']) for _, loc in pd.read_csv(f'{INGESTION_DATA_PATH}city_geo_loc.csv').iterrows()}\n    query = ''\n    count = 0\n    for death in death_data:\n        if death['location'] in insee_code_to_geo:\n            location = insee_code_to_geo[death['location']]\n\n            # convert date to postgres date format\n            birth_date = to_postgres_date(death['birth_date'])\n            death_date = to_postgres_date(death['death_date'])\n\n            # drop invalid data\n            import math\n            if math.isnan(location[0]) or math.isnan(location[1]) or birth_date is None or death_date is None:\n                continue\n            query += f\"INSERT INTO deaths VALUES ('{death['id']}', '{birth_date}', '{death_date}', '{location[0]}', '{location[1]}') ON CONFLICT DO NOTHING;\\n\"\n\n    # Save sql querys\n\n    with open(DEATH_INSERTION_QUERIES_PATH, \"w\") as f : f.write(query)\n    print(\"Created SQL query\")\n\ndef _death_emptiness_check():\n        with open(DEATH_INSERTION_QUERIES_PATH, \"r\") as f : \n            # check if file is empty\n            file_content = f.read()\n            # check if file content is empty\n            if len(file_content) == 0:\n                return staging_end.task_id\n            else:\n                return store_deaths_in_postgres.task_id\n\ndef _clean_tmp_death_files():\n    r = get_redis_client()\n    result = r.lrange('death_raw', 0, -1)\n    for _ in result : r.lpop('death_raw')\n    import os\n    if (os.path.exists(DEATH_INSERTION_QUERIES_PATH)):\n        os.remove(DEATH_INSERTION_QUERIES_PATH)\n\n\n\ndef pull_thermal_plants_data():\n    import json\n    thermal_resources = json.load(\n        open(f'{INGESTION_DATA_PATH}thermal_plants.json', 'r'))\n    import requests\n    for resource in thermal_resources['resources']:\n        if resource['format'] == 'csv':\n            response = requests.get(resource['latest'])\n            if response.status_code == 200:\n                with open(f'{INGESTION_DATA_PATH}thermal_plants_.csv', 'w') as outfile:\n                    outfile.write(response.content.decode(\"utf-8\"))\n            else:\n                print(\n                    f'Failed to get thermal plants resource')\n    print('Could not file resource in csv format')\n\ndef _create_plant_persist_sql_query():\n    df_thermal = pd.read_csv('dags/data/staging/thermal_plants_clean.csv')\n    # drop duplicate values of in the plant column of df_thermal\n    df_thermal.drop_duplicates(subset=['plant'], inplace=True)\n    \n    df_nuclear = pd.read_csv('dags/data/staging/nuclear_clean_datas.csv')\n    # drop duplicate values of in the plant column of df_nuclear\n    df_nuclear.drop_duplicates(subset=['plant'], inplace=True)\n    \n    \n    query = ''\n    import hashlib\n    for _, plant in df_thermal.iterrows():\n        id = hashlib.sha1(str(plant).encode()).hexdigest()\n        start_date = to_postgres_date(plant['start_date'])\n        print('startdate '+ start_date)\n        if start_date is None:\n            continue\n        position = plant['position'].split(',')\n        query += f\"INSERT INTO power_plants VALUES ('{id}', '{plant['plant']}', 'THERMAL', '{plant['fuel']}', '{start_date}', '{plant['power (MW)']}', '{position[0]}', '{position[1]}') ON CONFLICT DO NOTHING;\\n\"\n    for _, plant in df_nuclear.iterrows():\n        id = hashlib.sha1(str(plant).encode()).hexdigest()\n        start_date = to_postgres_date(plant['start_date'])\n        if start_date is None:\n            continue\n        position = plant['position'].split(',')\n        query += f\"INSERT INTO power_plants VALUES ('{id}', '{plant['plant']}', 'NUCLEAR', '{plant['fuel']}', '{start_date}', '{plant['power (MW)']}', '{position[0]}', '{position[1]}') ON CONFLICT DO NOTHING;\\n\"\n    \n    # Save sql querys\n    import os\n    if (os.path.exists(PLANT_INSERTION_QUERIES_PATH)):\n        os.remove(PLANT_INSERTION_QUERIES_PATH)\n    with open(PLANT_INSERTION_QUERIES_PATH, \"w\") as f:\n        f.write(query)\n    print(\"Created SQL query\")\n\ndef pull_death_file_list():\n    import requests\n    try:\n        data = requests.get(GET_DEATH_DATASET_URL).json()\n    except:\n        print('An error occurred when pulling the death file list')\n    import json\n    json_object = json.dumps(data['resources'])\n    with open(f'{INGESTION_DATA_PATH}death_resources.json', 'w') as outfile:\n        outfile.write(json_object)\n    print('An error occurred when saving the death list list')\n\n\ndef pull_all_death_files(max_resource=5):\n    import json\n    death_resources = json.load(\n        open(f'{INGESTION_DATA_PATH}death_resources.json', 'r'))\n    import requests\n    count = 0\n    for resource in death_resources:\n        count += 1\n        if count > max_resource:\n            print(f'Acquired the maximum of {max_resource} resources')\n            break\n\n        # pull the latest resource data\n        response = requests.get(resource['latest'])\n        if response.status_code == 200:\n            with open(f'{INGESTION_DATA_PATH}death_{resource[\"title\"]}', 'w') as outfile:\n                outfile.write(response.content.decode(\"utf-8\"))\n        else:\n            print(\n                f'Failed to get resource: {resource[\"title\"]} at url {resource[\"latest\"]}')\n\n\ndef pull_nuclear_plants():\n    import json\n    response = json.load(open(f'{INGESTION_DATA_PATH}nuclear_plants.json', 'r'))\n    import requests\n    for resource in response['resources']:\n        if resource['format'] == 'csv':\n            csv_resource = requests.get(resource['latest'])\n            if csv_resource.status_code == 200:\n                with open(f'{INGESTION_DATA_PATH}nuclear.csv', 'w') as outfile:\n                    outfile.write(csv_resource.content.decode(\"utf-8\"))\n            else:\n                print(f'Failed to extract nuclear plant data')\n            return\n    print('Could not file resource in csv format')\n\n# Operator definition\n# ===================\n\n\nwith TaskGroup(\"ingestion_pipeline\",\"data ingestion step\",dag=global_dag) as ingestion_pipeline:\n    start = DummyOperator(\n        task_id='start',\n        dag=global_dag,\n    )\n\n    get_nuclear_json = BashOperator(\n        task_id='get_nuclear_json',\n        dag=global_dag,\n        bash_command=f'curl {GET_NUCLEAR_DATAET_URL} --output /opt/airflow/{INGESTION_DATA_PATH}/nuclear_plants.json',\n    )\n\n    get_nuclear_data = PythonOperator(\n        task_id='get_nuclear_data',\n        dag=global_dag,\n        python_callable=pull_nuclear_plants,\n        op_kwargs={},\n        trigger_rule='all_success',\n        depends_on_past=False,\n    )\n\n    get_thermal_json = BashOperator(\n        task_id='get_thermal_plants_json',\n        dag=global_dag,\n        bash_command=f'curl {GET_THERMAL_DATASET_URL} --output /opt/airflow/{INGESTION_DATA_PATH}/thermal_plants.json',\n    )\n\n    get_thermal_data = PythonOperator(\n        task_id='get_thermal_data',\n        dag=global_dag,\n        python_callable=pull_thermal_plants_data,\n        op_kwargs={},\n        trigger_rule='all_success',\n        depends_on_past=False,\n    )\n\n    get_death_resource_list = PythonOperator(\n        task_id='get_death_resource_list',\n        dag=global_dag,\n        python_callable=pull_death_file_list,\n        op_kwargs={},\n        trigger_rule='all_success',\n        depends_on_past=False,\n    )\n\n    get_death_resources = PythonOperator(\n        task_id='get_death_resources',\n        dag=global_dag,\n        python_callable=pull_all_death_files,\n        op_kwargs={},\n        trigger_rule='all_success',\n        depends_on_past=False,\n    )\n\n    get_city_code_geo = BashOperator(\n        task_id='get_city_code_geo',\n        dag=global_dag,\n        bash_command=f'curl {CITY_GEO_DATASET_URL} --output /opt/airflow/{INGESTION_DATA_PATH}/city_geo_loc.csv',\n    )\n\n    end = DummyOperator(\n        task_id='end',\n        dag=global_dag,\n        trigger_rule='all_success'\n    )\n\n    start >> [get_city_code_geo, get_nuclear_json, get_death_resource_list, get_thermal_json]\n    get_nuclear_json >> get_nuclear_data\n    get_death_resource_list >> get_death_resources\n    get_thermal_json >> get_thermal_data\n    [get_nuclear_data, get_death_resources, get_thermal_data, get_city_code_geo] >> end\n\nwith TaskGroup(\"staging_pipeline\",\"data staging step\",dag=global_dag) as staging_pipeline:\n    start = DummyOperator(\n        task_id='start',\n        dag=global_dag,\n    )\n\n    staging_end = DummyOperator(\n        task_id='staging_end',\n        dag=global_dag,\n        trigger_rule='all_success'\n    )\n\n    # Postgres operators\n\n    create_death_table = PostgresOperator(\n        task_id='create_death_table',\n        dag=global_dag,\n        postgres_conn_id='postgres_default',\n        sql='sql/create_death_table.sql',\n        trigger_rule='none_failed',\n        autocommit=True,\n    )\n\n    create_power_plants_table = PostgresOperator(\n        task_id='create_power_plants_table',\n        dag=global_dag,\n        postgres_conn_id='postgres_default',\n        sql='sql/create_power_plant_table.sql',\n        trigger_rule='none_failed',\n        autocommit=True,\n    )\n\n    store_deaths_in_postgres = PostgresOperator(\n        task_id='store_deaths_in_postgres',\n        dag=global_dag,\n        postgres_conn_id='postgres_default',\n        sql=f'sql/tmp/{DEATH_INSERTION_QUERIES}',\n        trigger_rule='none_failed',\n        autocommit=True,\n    )\n\n    store_plants_in_postgres = PostgresOperator(\n        task_id='store_plants_in_postgres',\n        dag=global_dag,\n        postgres_conn_id='postgres_default',\n        sql=f'sql/tmp/{PLANT_INSERTION_QUERIES}',\n        trigger_rule='none_failed',\n        autocommit=True,\n    )\n\n    # Python operators\n\n    load_data_from_ingestion = PythonOperator(\n        task_id='load_data_from_ingestion',\n        dag=global_dag,\n        python_callable=_load_data_from_ingestion,\n        op_kwargs={},\n        depends_on_past=False,\n    )\n\n    cleanse_death_data = PythonOperator(\n        task_id='cleanse_death_data',\n        dag=global_dag,\n        python_callable=_cleanse_death_data,\n        op_kwargs={},\n        depends_on_past=False,\n    )\n\n    clean_tmp_death_files = PythonOperator(\n        task_id='clean_tmp_death_files',\n        dag=global_dag,\n        python_callable=_clean_tmp_death_files,\n        op_kwargs={},\n        depends_on_past=False,\n    )\n\n    import_thermal_clean_data = PythonOperator(\n        task_id='import_thermal_clean_data',\n        dag=global_dag,\n        python_callable=_import_thermal_clean_data,\n        op_kwargs={},\n        depends_on_past=False,\n    )\n\n    import_nuclear_clean_data = PythonOperator(\n        task_id='import_nuclear_clean_data',\n        dag=global_dag,\n        python_callable=_import_nuclear_clean_data,\n        op_kwargs={},\n        depends_on_past=False,\n    )\n\n    create_plant_persist_sql_query = PythonOperator(\n        task_id='create_plant_persist_sql_query',\n        dag=global_dag,\n        python_callable=_create_plant_persist_sql_query,\n        op_kwargs={},\n        trigger_rule='all_success',\n        depends_on_past=False,\n    )\n\n    # Python Branch Operators\n\n    death_emptiness_check = BranchPythonOperator(\n        task_id='death_emptiness_check',\n        dag=global_dag,\n        python_callable=_death_emptiness_check,\n        op_kwargs={\n            'previous_epoch': '{{ prev_execution_date.int_timestamp }}',\n    \"output_folder\": \"/opt/airflow/dags\"\n        },\n        trigger_rule='all_success',\n    )\n\n    start >> [import_nuclear_clean_data, import_thermal_clean_data,create_power_plants_table, create_death_table]\n    create_death_table >> load_data_from_ingestion >> cleanse_death_data >> death_emptiness_check\n    death_emptiness_check >> [staging_end, store_deaths_in_postgres]\n    store_deaths_in_postgres >> clean_tmp_death_files\n    import_nuclear_clean_data >> import_thermal_clean_data >> create_power_plants_table >> create_plant_persist_sql_query >> store_plants_in_postgres\n    [clean_tmp_death_files, death_emptiness_check,store_plants_in_postgres] >> staging_end\n\nstart_global = DummyOperator(\n    task_id='start_global',\n    dag=global_dag,\n    trigger_rule='all_success'\n)\n\nend_global = DummyOperator(\n    task_id='end_global',\n    dag=global_dag,\n    trigger_rule='all_success'\n)\n\nstart_global >> ingestion_pipeline >> staging_pipeline >> end_global\n"
    },
    {
      "version": "earliest",
      "commit": "e1ca9337ca1dfb213a85e5f76f36dd2cd6f9fb83",
      "code": "import airflow\nimport datetime\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.task_group import TaskGroup\n\nDEATH_DATASET_ID = '5de8f397634f4164071119c5'\nTHERMAL_DATASET_ID = '63587afb1cc488641390f68e'\nNUCLEAR_DATASET_ID = '63587afc1e8e90e9ce487174'\nINGESTION_DATA_PATH = 'dags/data/ingestion/'\nGET_DEATH_DATASET_URL = f'https://www.data.gouv.fr/api/1/datasets/{DEATH_DATASET_ID}/'\nGET_THERMAL_DATASET_URL = f'https://www.data.gouv.fr/api/1/datasets/{THERMAL_DATASET_ID}/'\nGET_NUCLEAR_DATAET_URL = f'https://www.data.gouv.fr/api/1/datasets/{NUCLEAR_DATASET_ID}/'\n# DAG definition\n\ndefault_args_dict = {\n    'start_date': airflow.utils.dates.days_ago(0),\n    'concurrency': 1,\n    'schedule_interval': None,\n    'retries': 1,\n    'retry_delay': datetime.timedelta(minutes=5),\n}\n\nglobal_dag = DAG(\n    dag_id='global_dag',\n    default_args=default_args_dict,\n    catchup=False,\n)\n\n# Python functions\n# ===================\n\n\ndef pull_thermal_plants_data():\n    import json\n    thermal_resources = json.load(\n        open(f'{INGESTION_DATA_PATH}thermal_plants.json', 'r'))\n    import requests\n    for resource in thermal_resources['resources']:\n        if resource['format'] == 'csv':\n            response = requests.get(resource['latest'])\n            if response.status_code == 200:\n                with open(f'dags/data/ingestion/thermal_plants_{resource[\"title\"]}.csv', 'w') as outfile:\n                    outfile.write(response.content.decode(\"utf-8\"))\n            else:\n                print(\n                    f'Failed to get thermal plants resource')\n    print('Could not file resource in csv format')\n\ndef pull_death_file_list():\n    import requests\n    try:\n        data = requests.get(GET_DEATH_DATASET_URL).json()\n    except:\n        print('An error occurred when pulling the death file list')\n    import json\n    json_object = json.dumps(data['resources'])\n    with open(f'{INGESTION_DATA_PATH}death_resources.json', 'w') as outfile:\n        outfile.write(json_object)\n    print('An error occurred when saving the death list list')\n\n\ndef pull_all_death_files(max_resource=5):\n    import json\n    death_resources = json.load(\n        open(f'{INGESTION_DATA_PATH}death_resources.json', 'r'))\n    import requests\n    count = 0\n    for resource in death_resources:\n        count += 1\n        if count > max_resource:\n            print(f'Acquired the maximum of {max_resource} resources')\n            break\n\n        # pull the latest resource data\n        response = requests.get(resource['latest'])\n        if response.status_code == 200:\n            with open(f'{INGESTION_DATA_PATH}death_{resource[\"title\"]}', 'w') as outfile:\n                outfile.write(response.content.decode(\"utf-8\"))\n        else:\n            print(\n                f'Failed to get resource: {resource[\"title\"]} at url {resource[\"latest\"]}')\n\n\ndef pull_nuclear_plants():\n    import json\n    response = json.load(open(f'{INGESTION_DATA_PATH}nuclear_plants.json', 'r'))\n    import requests\n    for resource in response['resources']:\n        if resource['format'] == 'csv':\n            csv_resource = requests.get(resource['latest'])\n            if csv_resource.status_code == 200:\n                with open(f'{INGESTION_DATA_PATH}nuclear_{resource[\"last_modified\"]}.csv', 'w') as outfile:\n                    outfile.write(csv_resource.content.decode(\"utf-8\"))\n            else:\n                print(f'Failed to extract nuclear plant data')\n            return\n    print('Could not file resource in csv format')\n\n# Operator definition\n# ===================\n\n\nwith TaskGroup(\"ingestion_pipeline\",\"data ingestion step\",dag=global_dag) as ingestion_pipeline:\n    start = DummyOperator(\n        task_id='start',\n        dag=global_dag,\n    )\n\n    get_nuclear_json = BashOperator(\n        task_id='get_nuclear_json',\n        dag=global_dag,\n        bash_command=f'curl {GET_NUCLEAR_DATAET_URL} --output /opt/airflow/{INGESTION_DATA_PATH}/nuclear_plants.json',\n    )\n\n    get_nuclear_data = PythonOperator(\n        task_id='get_nuclear_data',\n        dag=global_dag,\n        python_callable=pull_nuclear_plants,\n        op_kwargs={},\n        trigger_rule='all_success',\n        depends_on_past=False,\n    )\n\n    get_thermal_json = BashOperator(\n        task_id='get_thermal_plants_json',\n        dag=global_dag,\n        bash_command=f'curl {GET_THERMAL_DATASET_URL} --output /opt/airflow/{INGESTION_DATA_PATH}/thermal_plants.json',\n    )\n\n    get_thermal_data = PythonOperator(\n        task_id='get_thermal_data',\n        dag=global_dag,\n        python_callable=pull_thermal_plants_data,\n        op_kwargs={},\n        trigger_rule='all_success',\n        depends_on_past=False,\n    )\n\n\n    get_death_resource_list = PythonOperator(\n        task_id='get_death_resource_list',\n        dag=global_dag,\n        python_callable=pull_death_file_list,\n        op_kwargs={},\n        trigger_rule='all_success',\n        depends_on_past=False,\n    )\n\n    get_death_resources = PythonOperator(\n        task_id='get_death_resources',\n        dag=global_dag,\n        python_callable=pull_all_death_files,\n        op_kwargs={},\n        trigger_rule='all_success',\n        depends_on_past=False,\n    )\n\n    end = DummyOperator(\n        task_id='end',\n        dag=global_dag,\n        trigger_rule='all_success'\n    )\n\n    start >> [get_nuclear_json, get_death_resource_list, get_thermal_json]\n    get_nuclear_json >> get_nuclear_data\n    get_death_resource_list >> get_death_resources\n    get_thermal_json >> get_thermal_data\n    [get_nuclear_data, get_death_resources, get_thermal_data] >> end\n\nstart_global = DummyOperator(\n    task_id='start_global',\n    dag=global_dag,\n    trigger_rule='all_success'\n)\n\nend_global = DummyOperator(\n    task_id='end_global',\n    dag=global_dag,\n    trigger_rule='all_success'\n)\n\nstart_global >> ingestion_pipeline >> end_global\n"
    }
  ],
  "sampled_time": "2025-08-22T20:24:09.576568Z",
  "analysis": {
    "is_valid_dag_file": true,
    "is_production_dag": true,
    "is_airflow_2": true,
    "processing_type": "batch",
    "has_streaming_operators": false,
    "has_ml_operators": false,
    "topology": {
      "pattern": "staged_etl",
      "has_sensors": false,
      "has_branches": true,
      "has_fan_out": true,
      "has_fan_in": true,
      "estimated_max_parallel_width": 4,
      "estimated_branch_depth": 1,
      "has_cycles": false,
      "has_subdags": false,
      "has_task_groups": true
    },
    "tasks": {
      "total_count": 15,
      "operator_types": [
        "DummyOperator",
        "BashOperator",
        "PythonOperator",
        "BranchPythonOperator",
        "PostgresOperator"
      ],
      "has_dynamic_mapping": false,
      "has_external_task_sensor": false
    },
    "description": "ETL pipeline for processing death records and power plant data from French government APIs, with data ingestion, cleansing, and loading to PostgreSQL",
    "complexity_score": 7
  }
}