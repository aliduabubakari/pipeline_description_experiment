{
    "unique_key": "data_quality:customer_data_quality_scoring_dag.py",
    "repo": "synthetic-data-engineering",
    "repo_url": "https://github.com/internal/data-processing-scripts",
    "stars": 0,
    "license": "MIT",
    "file_path": "dags/data_quality/customer_data_quality_scoring_dag.py",
    "detected_airflow_version": "2.x",
    "sampled_versions": [
      {
        "version": "1.0",
        "commit": "airflow_implementation",
        "code": "import pandas as pd\nimport numpy as np\nimport random\nfrom datetime import datetime, timedelta\nimport json\nimport os\nimport warnings\nfrom pathlib import Path\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.utils.dates import days_ago\n\n# Suppress warnings for cleaner logs\nwarnings.filterwarnings('ignore')\n\n# Default arguments\ndefault_args = {\n    'owner': 'data_engineering',\n    'depends_on_past': False,\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Constants\nTEMP_DIR = '/tmp/airflow_dq_processing'\nos.makedirs(TEMP_DIR, exist_ok=True)\n\n@DAG(\n    dag_id='customer_dq_scoring_pipeline',\n    default_args=default_args,\n    description='Generates, cleans, and scores customer data quality',\n    schedule_interval='@daily',\n    start_date=days_ago(1),\n    catchup=False,\n    tags=['data_quality', 'scoring', 'etl']\n)\ndef customer_dq_scoring_dag():\n\n    # ========================================================================\n    # HELPER FUNCTIONS (Logic moved from original script)\n    # ========================================================================\n    \n    def logic_generate_data(num_rows):\n        # (Condensed generation logic for brevity in DAG)\n        customer_ids = [f\"CUST{str(i).zfill(6)}\" for i in range(1, num_rows + 1)]\n        start_date = datetime(2018, 1, 1)\n        \n        # Helper for random dates\n        def rand_date():\n            return start_date + timedelta(days=random.randint(0, 2000))\n\n        data = {\n            \"customer_id\": customer_ids,\n            \"first_name\": [random.choice([\"John\", \"Jane\", \"mary\", \"ROBERT\"]) for _ in range(num_rows)],\n            \"last_name\": [random.choice([\"Smith\", \"Johnson\", \"williams\"]) for _ in range(num_rows)],\n            \"email\": [f\"user{i}@example.com\" if random.random() > 0.05 else \"invalid_email\" for i in range(num_rows)],\n            \"phone\": [f\"555-01{i:02d}\" for i in range(num_rows)],\n            \"signup_date\": [rand_date() for _ in range(num_rows)],\n            \"age\": [random.randint(18, 80) if random.random() > 0.02 else -5 for _ in range(num_rows)],\n            \"annual_income\": [random.randint(30000, 150000) for _ in range(num_rows)],\n            \"credit_score\": [random.randint(300, 850) for _ in range(num_rows)],\n            \"customer_segment\": [random.choice([\"Premium\", \"Standard\", \"\"]) for _ in range(num_rows)],\n            \"total_purchases\": [random.randint(1, 100) for _ in range(num_rows)],\n            \"last_purchase_amount\": [random.randint(50, 2000) for _ in range(num_rows)]\n        }\n        df = pd.DataFrame(data)\n        \n        # Introduce Missing Values\n        for col in [\"email\", \"age\", \"credit_score\"]:\n            mask = np.random.random(num_rows) < 0.08\n            df.loc[mask, col] = np.nan\n            \n        return df\n\n    # ========================================================================\n    # TASKS\n    # ========================================================================\n\n    @task\n    def generate_synthetic_data() -> str:\n        \"\"\"Generate raw data and save to parquet.\"\"\"\n        print(\"Generating synthetic customer records...\")\n        df = logic_generate_data(num_rows=3500)\n        output_path = os.path.join(TEMP_DIR, 'raw_customers.parquet')\n        df.to_parquet(output_path, index=False)\n        print(f\"Generated {len(df)} rows. Saved to {output_path}\")\n        return output_path\n\n    @task\n    def explore_raw_data(file_path: str):\n        \"\"\"Print EDA statistics for raw data.\"\"\"\n        df = pd.read_parquet(file_path)\n        print(\"RAW DATA STATS:\")\n        print(f\"Shape: {df.shape}\")\n        print(f\"Missing Values:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")\n\n    @task\n    def clean_data(file_path: str) -> str:\n        \"\"\"Apply cleaning logic to the dataframe.\"\"\"\n        print(\"Cleaning customer data...\")\n        df = pd.read_parquet(file_path)\n        \n        # Text standardization\n        for col in [\"first_name\", \"last_name\", \"customer_segment\"]:\n            df[col] = df[col].astype(str).str.strip().str.title().replace(['', 'Nan', 'None', 'N/A'], np.nan)\n            \n        # Imputation\n        df[\"customer_segment\"].fillna(\"Unknown\", inplace=True)\n        for col in [\"age\", \"annual_income\", \"credit_score\", \"total_purchases\"]:\n            df[col].fillna(df[col].median(), inplace=True)\n            \n        # Outlier Capping (Age 18-100)\n        df['age'] = df['age'].clip(18, 100)\n        \n        # Email Validation placeholder\n        df.loc[~df['email'].str.contains('@', na=False), 'email'] = 'invalid@placeholder.com'\n        df['email'].fillna('invalid@placeholder.com', inplace=True)\n        \n        output_path = os.path.join(TEMP_DIR, 'cleaned_customers.parquet')\n        df.to_parquet(output_path, index=False)\n        return output_path\n\n    @task\n    def calculate_quality_score(file_path: str) -> dict:\n        \"\"\"Calculate completeness, validity, and consistency scores.\"\"\"\n        print(\"Calculating DQ scores...\")\n        df = pd.read_parquet(file_path)\n        \n        # 1. Completeness\n        total_cells = df.size\n        non_null = df.notnull().sum().sum()\n        comp_score = (non_null / total_cells * 100) if total_cells > 0 else 0\n        \n        # 2. Validity (Simplified rules)\n        failed = 0\n        if not df['age'].between(18, 100).all(): failed += 1\n        if not (df['annual_income'] >= 0).all(): failed += 1\n        if not df['email'].str.contains('@').all(): failed += 1\n        \n        validity_score = ((len(df) - failed) / len(df)) * 100 # Simplification for aggregate\n        \n        # 3. Consistency (CV)\n        cv_scores = []\n        for col in ['age', 'annual_income', 'credit_score']:\n            if df[col].mean() > 0:\n                cv = (df[col].std() / df[col].mean()) * 100\n                cv_scores.append(max(0, 100 - min(cv, 100)))\n        consistency_score = np.mean(cv_scores) if cv_scores else 100\n        \n        # Overall\n        overall = (comp_score * 0.4) + (validity_score * 0.4) + (consistency_score * 0.2)\n        \n        scores = {\n            \"completeness\": round(comp_score, 2),\n            \"validity\": round(validity_score, 2),\n            \"consistency\": round(consistency_score, 2),\n            \"overall\": round(overall, 2)\n        }\n        print(f\"DQ Scores: {scores}\")\n        return scores\n\n    @task\n    def feature_engineering(file_path: str) -> str:\n        \"\"\"Add derived features.\"\"\"\n        df = pd.read_parquet(file_path)\n        \n        # Tenure\n        df['signup_date'] = pd.to_datetime(df['signup_date'])\n        df['tenure_days'] = (pd.Timestamp.now() - df['signup_date']).dt.days\n        \n        # Normalized Income\n        df['income_norm'] = (df['annual_income'] - df['annual_income'].min()) / (df['annual_income'].max() - df['annual_income'].min())\n        \n        # Value Score\n        df['value_score'] = (df['income_norm'] * 0.5) + (df['total_purchases'] * 0.5) # Simplified\n        \n        output_path = os.path.join(TEMP_DIR, 'final_customers.parquet')\n        df.to_parquet(output_path, index=False)\n        return output_path\n\n    @task\n    def save_and_report(data_path: str, scores: dict):\n        \"\"\"Save final CSV and JSON report.\"\"\"\n        df = pd.read_parquet(data_path)\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        \n        # Save CSV\n        csv_out = os.path.join(TEMP_DIR, f\"cleaned_customers_{timestamp}.csv\")\n        df.to_csv(csv_out, index=False)\n        \n        # Save Report\n        report_out = os.path.join(TEMP_DIR, f\"dq_report_{timestamp}.json\")\n        with open(report_out, 'w') as f:\n            json.dump(scores, f, indent=2)\n            \n        print(f\"Pipeline Finished. Quality Score: {scores['overall']}%\")\n        print(f\"Data saved to {csv_out}\")\n\n    # ========================================================================\n    # ORCHESTRATION\n    # ========================================================================\n    \n    # Data passing establishes the dependencies implicitly\n    raw_file = generate_synthetic_data()\n    \n    # Parallel branch for exploration (doesn't block cleaning)\n    explore_raw_data(raw_file)\n    \n    # Main Processing Chain\n    cleaned_file = clean_data(raw_file)\n    \n    # Branch: Score calculation\n    dq_scores = calculate_quality_score(cleaned_file)\n    \n    # Branch: Feature Engineering\n    final_data = feature_engineering(cleaned_file)\n    \n    # Join: Save everything\n    save_and_report(final_data, dq_scores)\n\n# Instantiate\ncustomer_dq_scoring_dag_instance = customer_dq_scoring_dag()"
      }
    ],
    "sampled_time": "2025-12-09T22:42:59.617282",
    "analysis": {
      "is_valid_dag_file": true,
      "is_production_dag": true,
      "is_airflow_2": true,
      "processing_type": "batch",
      "has_streaming_operators": false,
      "has_ml_operators": false,
      "topology": {
        "pattern": "linear",
        "has_sensors": false,
        "has_branches": true,
        "has_fan_out": false,
        "has_fan_in": true,
        "estimated_max_parallel_width": 2,
        "estimated_branch_depth": 1,
        "has_cycles": false,
        "has_subdags": false,
        "has_task_groups": false
      },
      "tasks": {
        "total_count": 6,
        "operator_types": [
          "PythonOperator (TaskFlow)"
        ],
        "has_dynamic_mapping": false,
        "has_external_task_sensor": false
      },
      "description": "An Airflow 2.x DAG that performs end-to-end data quality management on customer data. It generates synthetic data, performs rigorous cleaning and outlier handling, calculates quality metrics (Completeness, Validity, Consistency), engineers features, and exports a final quality report.",
      "complexity_score": 4
    }
  }