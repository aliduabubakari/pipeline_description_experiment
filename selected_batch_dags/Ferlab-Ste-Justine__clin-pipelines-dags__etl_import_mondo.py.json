{
  "unique_key": "Ferlab-Ste-Justine/clin-pipelines-dags:dags/etl_import_mondo.py",
  "repo": "Ferlab-Ste-Justine/clin-pipelines-dags",
  "repo_url": "https://github.com/Ferlab-Ste-Justine/clin-pipelines-dags",
  "stars": 0,
  "license": "Apache-2.0",
  "file_path": "dags/etl_import_mondo.py",
  "detected_airflow_version": "2.x",
  "sampled_versions": [
    {
      "version": "latest",
      "commit": "cec3f5297fc407657bc2daad128307f2ffa17b44",
      "code": "import logging\nimport re\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.param import Param\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.utils.context import Context\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom lib import config\nfrom lib.config import K8sContext, config_file, env, es_url, indexer_context\nfrom lib.operators.curl import CurlOperator\nfrom lib.operators.pipeline import PipelineOperator\nfrom lib.operators.spark import SparkOperator\nfrom lib.slack import Slack\nfrom lib.tasks import publish_index\nfrom lib.tasks.params_validate import validate_color\nfrom lib.utils import http_get, http_get_file\nfrom lib.utils_etl import (batch_id, color, obo_parser_spark_jar, skip_import,\n                           spark_jar)\nfrom lib.utils_s3 import get_s3_file_version, load_to_s3_with_version\n\nwith DAG(\n    dag_id='etl_import_mondo',\n    start_date=datetime(2022, 1, 1),\n    schedule=None,\n    params={\n        'color': Param('', type=['null', 'string']),\n        'spark_jar': Param('', type=['null', 'string']),\n        'obo_parser_spark_jar': Param('', type=['null', 'string']),\n    },\n    default_args={\n        'trigger_rule': TriggerRule.NONE_FAILED,\n        'on_failure_callback': Slack.notify_task_failure\n    },\n    max_active_tasks=1,\n    max_active_runs=1\n    ) as dag:\n\n    params_validate = validate_color(color())\n\n    def download(file, dest = None, **context):\n        url = 'https://github.com/monarch-initiative/mondo/releases'\n\n        destFile = file if dest is None else dest\n\n        s3 = S3Hook(config.s3_conn_id)\n        s3_bucket = f'cqgc-{env}-app-datalake'\n        s3_key = f'raw/landing/mondo/{destFile}'\n\n        # Get imported version\n        imported_ver = get_s3_file_version(s3, s3_bucket, s3_key)\n        logging.info(f'{file} imported version: {imported_ver}')\n\n        # Get latest version\n        html = http_get(url).text\n        latest_ver_search = re.search(f'/download/(v?.+)/{file}', html)\n\n        if latest_ver_search is None:\n            logging.error(f'Could not find source latest version for: {file}')\n            context['ti'].xcom_push(key=f'{destFile}.version', value=imported_ver)\n            raise AirflowSkipException()\n\n        latest_ver = latest_ver_search.group(1)\n        logging.info(f'{file} latest version: {latest_ver}')\n\n        # share the current version with other tasks\n        context['ti'].xcom_push(key=f'{destFile}.version', value=latest_ver)\n\n        # Skip task if up to date\n        if imported_ver == latest_ver:\n            raise AirflowSkipException()\n\n        # Download file\n        http_get_file(f'{url}/download/{latest_ver}/{file}', file)\n\n        # Upload file to S3\n        load_to_s3_with_version(s3, s3_bucket, s3_key, file, latest_ver)\n        logging.info(f'New {file} imported version: {latest_ver}')\n\n\n    # used to get the version, the obo-parser will download the file on its own again\n    download_mondo_terms = PythonOperator(\n        task_id='download_mondo_terms',\n        python_callable=download,\n        op_args=['mondo-base.obo', 'mondo.obo']\n    )\n\n    normalized_mondo_terms = SparkOperator(\n        task_id='normalized_mondo_terms',\n        name='etl-import-hpo-terms',\n        k8s_context=K8sContext.ETL,\n        spark_class='bio.ferlab.HPOMain',\n        spark_config='config-etl-medium',\n        spark_jar=obo_parser_spark_jar(),\n        arguments=[\n            'https://github.com/monarch-initiative/mondo/releases/download/{{ ti.xcom_pull(task_ids=\"download_mondo_terms\", key=\"mondo.obo.version\") }}/mondo-base.obo',\n            f'cqgc-{env}-app-datalake',\n            'public/mondo_terms',\n            'False',\n            'MONDO:0700096',\n        ],\n    )\n\n    index_mondo_terms = SparkOperator(\n        task_id='index_mondo_terms',\n        name='etl-index-terms',\n        k8s_context=indexer_context,\n        spark_class='bio.ferlab.clin.etl.es.Indexer',\n        spark_config='config-etl-singleton',\n        spark_jar=spark_jar(),\n        arguments=[\n            es_url, '', '',\n            'mondo',\n            '{{ ti.xcom_pull(task_ids=\"download_mondo_terms\", key=\"mondo.obo.version\") }}',\n            'mondo_terms_template.json',\n            'mondo_terms',\n            '1900-01-01 00:00:00',\n            f'config/{env}.conf',\n        ],\n    )\n\n    publish_mondo = publish_index.mondo('{{ ti.xcom_pull(task_ids=\"download_mondo_terms\", key=\"mondo.obo.version\") }}', color('_'), spark_jar(), task_id='publish_mondo')\n\n    slack = EmptyOperator(\n        task_id=\"slack\",\n        on_success_callback=Slack.notify_dag_completion,\n    )\n\n    chain(params_validate, download_mondo_terms, normalized_mondo_terms, index_mondo_terms, publish_mondo, slack)\n"
    },
    {
      "version": "earliest",
      "commit": "be7dc209587bea2e70826d7786cdb6210a5cd555",
      "code": "import logging\nimport re\nfrom datetime import datetime\n\nfrom airflow import DAG\nfrom airflow.exceptions import AirflowSkipException\nfrom airflow.models.baseoperator import chain\nfrom airflow.models.param import Param\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.utils.context import Context\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom lib import config\nfrom lib.config import K8sContext, config_file, env, es_url, indexer_context\nfrom lib.operators.curl import CurlOperator\nfrom lib.operators.pipeline import PipelineOperator\nfrom lib.operators.spark import SparkOperator\nfrom lib.slack import Slack\nfrom lib.tasks import publish_index\nfrom lib.tasks.params_validate import validate_color\nfrom lib.utils import http_get, http_get_file\nfrom lib.utils_etl import (batch_id, color, obo_parser_spark_jar, skip_import,\n                           spark_jar)\nfrom lib.utils_import import get_s3_file_version, load_to_s3_with_version\n\nwith DAG(\n    dag_id='etl_import_mondo',\n    start_date=datetime(2022, 1, 1),\n    schedule_interval=None,\n    params={\n        'color': Param('', type=['null', 'string']),\n        'spark_jar': Param('', type=['null', 'string']),\n        'obo_parser_spark_jar': Param('', type=['null', 'string']),\n    },\n    default_args={\n        'trigger_rule': TriggerRule.NONE_FAILED,\n        'on_failure_callback': Slack.notify_task_failure\n    },\n    max_active_tasks=1,\n    max_active_runs=1\n    ) as dag:\n\n    params_validate = validate_color(color())\n\n    def download(file, dest = None, **context):\n        url = 'https://github.com/monarch-initiative/mondo/releases'\n\n        destFile = file if dest is None else dest\n\n        s3 = S3Hook(config.s3_conn_id)\n        s3_bucket = f'cqgc-{env}-app-datalake'\n        s3_key = f'raw/landing/mondo/{destFile}'\n\n        # Get latest version\n        html = http_get(url).text\n        latest_ver = re.search(f'/download/(v.+)/{file}', html).group(1)\n        logging.info(f'{file} latest version: {latest_ver}')\n\n        # Get imported version\n        imported_ver = get_s3_file_version(s3, s3_bucket, s3_key)\n        logging.info(f'{file} imported version: {imported_ver}')\n\n        # share the current version with other tasks\n        context['ti'].xcom_push(key=f'{destFile}.version', value=latest_ver)\n\n        # Skip task if up to date\n        if imported_ver == latest_ver:\n            raise AirflowSkipException()\n\n        # Download file\n        http_get_file(f'{url}/download/{latest_ver}/{file}', file)\n\n        # Upload file to S3\n        load_to_s3_with_version(s3, s3_bucket, s3_key, file, latest_ver)\n        logging.info(f'New {file} imported version: {latest_ver}')\n\n\n    # used to get the version, the obo-parser will download the file on its own again\n    download_mondo_terms = PythonOperator(\n        task_id='download_mondo_terms',\n        python_callable=download,\n        op_args=['mondo-base.obo', 'mondo.obo'],\n        provide_context=True,\n    )\n\n    normalized_mondo_terms = SparkOperator(\n        task_id='normalized_mondo_terms',\n        name='etl-import-hpo-terms',\n        k8s_context=K8sContext.ETL,\n        spark_class='bio.ferlab.HPOMain',\n        spark_config='config-etl-medium',\n        spark_jar=obo_parser_spark_jar(),\n        arguments=[\n            'https://github.com/monarch-initiative/mondo/releases/download/{{ ti.xcom_pull(task_ids=\"download_mondo_terms\", key=\"mondo.obo.version\") }}/mondo.obo',\n            f'cqgc-{env}-app-datalake',\n            'public/mondo_terms',\n            'False',\n            '',\n        ],\n    )\n\n    index_mondo_terms = SparkOperator(\n        task_id='index_mondo_terms',\n        name='etl-index-terms',\n        k8s_context=indexer_context,\n        spark_class='bio.ferlab.clin.etl.es.Indexer',\n        spark_config='config-etl-singleton',\n        spark_jar=spark_jar(),\n        arguments=[\n            es_url, '', '',\n            f'clin_{env}' + color('_') + '_mondo', #clin_qa_green_hpo_v2024-01-01\n            '{{ ti.xcom_pull(task_ids=\"download_mondo_terms\", key=\"mondo.obo.version\") }}',\n            'mondo_terms_template.json',\n            'mondo_terms',\n            '1900-01-01 00:00:00',\n            f'config/{env}.conf',\n        ],\n    )\n\n    publish_mondo = publish_index.mondo('{{ ti.xcom_pull(task_ids=\"download_mondo_terms\", key=\"mondo.obo.version\") }}', color('_'), spark_jar(), task_id='publish_mondo')\n\n    slack = EmptyOperator(\n        task_id=\"slack\",\n        on_success_callback=Slack.notify_dag_completion,\n    )\n\n    chain(params_validate, download_mondo_terms, normalized_mondo_terms, index_mondo_terms, publish_mondo, slack)\n"
    }
  ],
  "sampled_time": "2025-08-22T21:13:13.837695Z",
  "analysis": {
    "is_valid_dag_file": true,
    "is_production_dag": true,
    "is_airflow_2": true,
    "processing_type": "batch",
    "has_streaming_operators": false,
    "has_ml_operators": false,
    "topology": {
      "pattern": "linear",
      "has_sensors": false,
      "has_branches": false,
      "has_fan_out": false,
      "has_fan_in": false,
      "estimated_max_parallel_width": 1,
      "estimated_branch_depth": 0,
      "has_cycles": false,
      "has_subdags": false,
      "has_task_groups": false
    },
    "tasks": {
      "total_count": 6,
      "operator_types": [
        "PythonOperator",
        "SparkOperator",
        "EmptyOperator"
      ],
      "has_dynamic_mapping": false,
      "has_external_task_sensor": false
    },
    "description": "ETL pipeline for importing Mondo ontology data - downloads Mondo OBO file, normalizes terms using Spark, indexes to Elasticsearch, publishes results, and sends Slack notification",
    "complexity_score": 4
  }
}