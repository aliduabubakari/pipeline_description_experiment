{
  "unique_key": "HaydarovAkbar/airflow_dag:main.py",
  "repo": "HaydarovAkbar/airflow_dag",
  "repo_url": "https://github.com/HaydarovAkbar/airflow_dag",
  "stars": 2,
  "license": "Apache-2.0",
  "file_path": "main.py",
  "detected_airflow_version": "2.x",
  "sampled_versions": [
    {
      "version": "latest",
      "commit": "55e3e8619fedf0bd0cda21b90ee366c3c073d44c",
      "code": "import pendulum\nimport datetime as dt\nfrom datetime import timedelta\n\nimport md_dwh\n\nfrom airflow.models import DAG\nfrom airflow.sensors.sql import SqlSensor\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.email_operator import EmailOperator\nfrom airflow.operators.python import get_current_context\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nfrom airflow.sensors.external_task_sensor import ExternalTaskSensor\nfrom airflow.providers.http.hooks.http import HttpHook\nfrom airflow.utils.task_group import TaskGroup\nfrom airflow.exceptions import AirflowException\n\n\nimport logging\nimport psycopg2\n\nlocal_tz = pendulum.timezone('Asia/Tashkent')\n\n#подключаемся к нужным базам (источник/приемник)\npar = md_dwh.get_dsn_parameters()\n\nSRC_SAP = HttpHook(method='POST', http_conn_id='sap_conn')\n\ndefault_args = {\n    'owner': 'dwh_l2_to_l2',\n    'start_date': dt.datetime(2024, 12, 22),\n    'retries': 0,\n    'email': ['test@gmail.com'],\n    'email_on_failure': True,\n    'email_on_retry': False\n}\n\n\ndef p_workflow_registration(**kwargs):\n    ti = kwargs['ti']\n    str_load_id = ti.xcom_pull(key='load_id', task_ids='get_load_id')\n    context = get_current_context()\n    dag_id = context['dag_run'].dag_id\n    task_id = context['dag_run'].run_id\n    log_url = context['task_instance'].log_url\n    md_dwh.workflow_registration(dag_id, dag_id, 'l2')\n    md_dwh.workflow_set_data_load_session(str_load_id, local_tz.convert(context['logical_date']).to_date_string(), 'l2',\n                                          'start')\n    md_dwh.workflow_set_reg_data_job_session(str_load_id, task_id, dag_id, 'start')\n    md_dwh.workflow_set_data_session_log(task_id, 'i', 'start - > task_id = ' +\n                                         task_id + ', dag_id = ' + dag_id + ', log_url = ' + log_url)\n\n\ndef p_end(**kwargs):\n    ti = kwargs['ti']\n    str_load_id = ti.xcom_pull(key='load_id', task_ids='get_load_id')\n    context = get_current_context()\n    dag_id = context['dag_run'].dag_id\n    task_id = context['dag_run'].run_id\n    log_url = context['task_instance'].log_url\n    md_dwh.workflow_update_data_load_session(str_load_id, 's')\n    md_dwh.workflow_set_reg_data_job_session(str_load_id, task_id, dag_id, 's')\n    md_dwh.workflow_set_data_session_log(task_id, 'i', 'successful - > task_id = '\n                                         + task_id + ', dag_id = ' + dag_id + ', log_url = ' + log_url)\n\n\n# def p_create_dwh_flag():\n#     context = get_current_context()\n#     dag_id = context['dag_run'].dag_id\n#     md_dwh.create_dwh_flag('загрузка datasets из dwh l2 в dwh l2', 'l2_datasets_load_successful',\n#                            local_tz.convert(context['logical_date']).to_date_string(), 'l2', 'airflow',\n#                            'загрузка datasets из dwh l2 в dwh l2', dag_id)\n\n\n# отправка флага готовности в систему SAP\ndef p_send_flg_to_sap(**kwargs):\n    context = get_current_context()\n\n    sql_query = f'''\n        select count(*) as count\n        from wk_export.ds_client_segmentation_last_v\n    '''\n\n    count = None\n    conn = psycopg2.connect(**par)\n    cursor = conn.cursor()\n\n    try:\n        logging.info('fetch the count of rows from wk_export.ds_client_segmentation')\n        cursor.execute(sql_query)\n        count = str(cursor.fetchone()[0])\n        conn.commit()\n        cursor.close()\n        conn.close()\n        logging.info('the count of rows is fetched')\n    except (Exception, psycopg2.Error) as error:\n        cursor.close()\n        conn.close()\n        logging.info('Error: %s' % error)\n        raise AirflowException(f\" Error: %s\" % error)\n\n    send_data = {\n        \"calc_ended\": \"X\",\n        \"calc_date\": str(dt.date.today()),\n        \"calc_rows\": count,\n        \"logical_date\": local_tz.convert(context['logical_date']).to_date_string(),\n        \"source_sys_cd\": \"DWH\",\n        \"code_flg\": \"l2_to_SAP_successful\"\n    }\n\n    response = SRC_SAP.run(\n        json=send_data,\n        extra_options={'check_response': True}\n    )\n\n    logging.info(send_data)\n    logging.info(response)\n\n\nwith DAG(dag_id='WF_MAIN_DATASETS_LOAD_L2_TO_L2',\n         description='WF_MAIN_DATASETS_LOAD_L2_TO_L2',\n         # schedule_interval='0 3 * * *',\n         schedule_interval=None,\n         catchup=False,\n         max_active_runs=20,\n         default_args=default_args) as dag:\n\n    get_load_id = PythonOperator(\n        task_id='get_load_id',\n        python_callable=md_dwh.get_date_for_session_dwh,\n        dag=dag)\n    workflow_registration = PythonOperator(\n        task_id='workflow_registration',\n        python_callable=p_workflow_registration,\n        dag=dag)\n    # create_dwh_flag = PythonOperator(\n    #     task_id='create_dwh_flag',\n    #     python_callable=p_create_dwh_flag,\n    #     dag=dag)\n    end_task = PythonOperator(\n        task_id='end',\n        python_callable=p_end,\n        dag=dag)\n\n    wait_for_l2_full_load = SqlSensor(\n        task_id='wait_for_l2_full_load',\n        conn_id='dwh',\n        sql=\"select * from md.dwh_flag where flag_cd = 'l1_to_l2_load_successfull' \"\n            \"and bussines_date = current_date - 1 \",\n        parameters={\n            'flag_name': 'загрузка данных из dwh l1 в dwh l2'\n        },\n        fail_on_empty=False,\n        poke_interval=60\n    )\n\n    sensor_success_end = ExternalTaskSensor(\n        task_id='wait_for_success_end',\n        external_dag_id='WF_MAIN_DATASETS_LOAD_L2_TO_L2',\n        external_task_id='end',\n        mode='poke',\n        poke_interval=100,\n        execution_delta=timedelta(days=1)\n    )\n\n    run_sys_kill_all_session_pg = TriggerDagRunOperator(\n        task_id='run_sys_kill_all_session_pg',\n        trigger_dag_id='sys_kill_all_session_pg',\n        reset_dag_run=True,\n        wait_for_completion=True,\n        execution_date='{{ dag_run.logical_date }}'\n    )\n\n    run_wf_data_preparation_for_reports = TriggerDagRunOperator(\n        task_id='run_wf_data_preparation_for_reports',\n        trigger_dag_id='wf_data_preparation_for_reports',\n        reset_dag_run=True,\n        wait_for_completion=True,\n        execution_date='{{ dag_run.logical_date }}', pool_slots=1, pool='dwh_l2'\n    )\n\n    email_failure = EmailOperator(\n        task_id='email_on_failure',\n        to=['test@gmail.com'],\n        subject='Airflow DAG Execution Failure',\n        html_content=f'Your Airflow {dag.dag_id} encountered a failure.',\n        trigger_rule='one_failed',\n        dag=dag\n    )\n\n    with TaskGroup(group_id='segmentation_group') as segmentation_group:\n        load_ds_client_segmentation = TriggerDagRunOperator(\n            task_id='load_ds_client_segmentation',\n            trigger_dag_id='l1_to_l2_p_load_data_ds_client_segmentation_full',\n            reset_dag_run=True,\n            wait_for_completion=True,\n            execution_date='{{ dag_run.logical_date }}')\n        send_flg_to_sap = PythonOperator(\n            task_id='send_flg_to_sap',\n            python_callable=p_send_flg_to_sap,\n            dag=dag)\n\n        load_ds_client_segmentation >> send_flg_to_sap\n\n\n    wait_for_l2_full_load >> get_load_id >> workflow_registration >> sensor_success_end >> run_sys_kill_all_session_pg >> \\\n       [run_wf_data_preparation_for_reports,segmentation_group] >> end_task >> email_failure\n"
    },
    {
      "version": "earliest",
      "commit": "4f143f186aafaa40e106846c27d0f59ed8557dee",
      "code": "import pendulum\nimport datetime as dt\nfrom datetime import timedelta\n\nimport md_dwh\n\nfrom airflow.models import DAG\nfrom airflow.sensors.sql import SqlSensor\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.email_operator import EmailOperator\nfrom airflow.operators.python import get_current_context\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nfrom airflow.sensors.external_task_sensor import ExternalTaskSensor\nfrom airflow.providers.http.hooks.http import HttpHook\nfrom airflow.utils.task_group import TaskGroup\nfrom airflow.exceptions import AirflowException\nimport logging\nimport psycopg2\n\nlocal_tz = pendulum.timezone('Asia/Tashkent')\n\n#подключаемся к нужным базам (источник/приемник)\npar = md_dwh.get_dsn_parameters()\n\nSRC_SAP = HttpHook(method='POST', http_conn_id='sap_conn')\n\ndefault_args = {\n    'owner': 'dwh_l2_to_l2',\n    'start_date': dt.datetime(2023, 12, 22),\n    'retries': 0,\n    'email': ['b_alimov@ipakyulibank.uz'],\n    'email_on_failure': True,\n    'email_on_retry': False\n}\n\n\ndef p_workflow_registration(**kwargs):\n    ti = kwargs['ti']\n    str_load_id = ti.xcom_pull(key='load_id', task_ids='get_load_id')\n    context = get_current_context()\n    dag_id = context['dag_run'].dag_id\n    task_id = context['dag_run'].run_id\n    log_url = context['task_instance'].log_url\n    md_dwh.workflow_registration(dag_id, dag_id, 'l2')\n    md_dwh.workflow_set_data_load_session(str_load_id, local_tz.convert(context['logical_date']).to_date_string(), 'l2',\n                                          'start')\n    md_dwh.workflow_set_reg_data_job_session(str_load_id, task_id, dag_id, 'start')\n    md_dwh.workflow_set_data_session_log(task_id, 'i', 'start - > task_id = ' +\n                                         task_id + ', dag_id = ' + dag_id + ', log_url = ' + log_url)\n\n\ndef p_end(**kwargs):\n    ti = kwargs['ti']\n    str_load_id = ti.xcom_pull(key='load_id', task_ids='get_load_id')\n    context = get_current_context()\n    dag_id = context['dag_run'].dag_id\n    task_id = context['dag_run'].run_id\n    log_url = context['task_instance'].log_url\n    md_dwh.workflow_update_data_load_session(str_load_id, 's')\n    md_dwh.workflow_set_reg_data_job_session(str_load_id, task_id, dag_id, 's')\n    md_dwh.workflow_set_data_session_log(task_id, 'i', 'successful - > task_id = '\n                                         + task_id + ', dag_id = ' + dag_id + ', log_url = ' + log_url)\n\n\n# def p_create_dwh_flag():\n#     context = get_current_context()\n#     dag_id = context['dag_run'].dag_id\n#     md_dwh.create_dwh_flag('загрузка datasets из dwh l2 в dwh l2', 'l2_datasets_load_successful',\n#                            local_tz.convert(context['logical_date']).to_date_string(), 'l2', 'airflow',\n#                            'загрузка datasets из dwh l2 в dwh l2', dag_id)\n\n\n# отправка флага готовности в систему SAP\ndef p_send_flg_to_sap(**kwargs):\n    context = get_current_context()\n\n    sql_query = f'''\n        select count(*) as count\n        from wk_export.ds_client_segmentation_last_v\n    '''\n\n    count = None\n    conn = psycopg2.connect(**par)\n    cursor = conn.cursor()\n\n    try:\n        logging.info('fetch the count of rows from wk_export.ds_client_segmentation')\n        cursor.execute(sql_query)\n        count = str(cursor.fetchone()[0])\n        conn.commit()\n        cursor.close()\n        conn.close()\n        logging.info('the count of rows is fetched')\n    except (Exception, psycopg2.Error) as error:\n        cursor.close()\n        conn.close()\n        logging.info('Error: %s' % error)\n        raise AirflowException(f\" Error: %s\" % error)\n\n    send_data = {\n        \"calc_ended\": \"X\",\n        \"calc_date\": str(dt.date.today()),\n        \"calc_rows\": count,\n        \"logical_date\": local_tz.convert(context['logical_date']).to_date_string(),\n        \"source_sys_cd\": \"DWH\",\n        \"code_flg\": \"l2_to_SAP_successful\"\n    }\n\n    response = SRC_SAP.run(\n        json=send_data,\n        extra_options={'check_response': True}\n    )\n\n    logging.info(send_data)\n    logging.info(response)\n\n\nwith DAG(dag_id='WF_MAIN_DATASETS_LOAD_L2_TO_L2',\n         description='WF_MAIN_DATASETS_LOAD_L2_TO_L2',\n         # schedule_interval='0 3 * * *',\n         schedule_interval=None,\n         catchup=False,\n         max_active_runs=20,\n         default_args=default_args) as dag:\n\n    get_load_id = PythonOperator(\n        task_id='get_load_id',\n        python_callable=md_dwh.get_date_for_session_dwh,\n        dag=dag)\n    workflow_registration = PythonOperator(\n        task_id='workflow_registration',\n        python_callable=p_workflow_registration,\n        dag=dag)\n    # create_dwh_flag = PythonOperator(\n    #     task_id='create_dwh_flag',\n    #     python_callable=p_create_dwh_flag,\n    #     dag=dag)\n    end_task = PythonOperator(\n        task_id='end',\n        python_callable=p_end,\n        dag=dag)\n\n    wait_for_l2_full_load = SqlSensor(\n        task_id='wait_for_l2_full_load',\n        conn_id='dwh',\n        sql=\"select * from md.dwh_flag where flag_cd = 'l1_to_l2_load_successfull' \"\n            \"and bussines_date = current_date - 1 \",\n        parameters={\n            'flag_name': 'загрузка данных из dwh l1 в dwh l2'\n        },\n        fail_on_empty=False,\n        poke_interval=60\n    )\n\n    sensor_success_end = ExternalTaskSensor(\n        task_id='wait_for_success_end',\n        external_dag_id='WF_MAIN_DATASETS_LOAD_L2_TO_L2',\n        external_task_id='end',\n        mode='poke',\n        poke_interval=100,\n        execution_delta=timedelta(days=1)\n    )\n\n    run_sys_kill_all_session_pg = TriggerDagRunOperator(\n        task_id='run_sys_kill_all_session_pg',\n        trigger_dag_id='sys_kill_all_session_pg',\n        reset_dag_run=True,\n        wait_for_completion=True,\n        execution_date='{{ dag_run.logical_date }}'\n    )\n\n    run_wf_data_preparation_for_reports = TriggerDagRunOperator(\n        task_id='run_wf_data_preparation_for_reports',\n        trigger_dag_id='wf_data_preparation_for_reports',\n        reset_dag_run=True,\n        wait_for_completion=True,\n        execution_date='{{ dag_run.logical_date }}', pool_slots=1, pool='dwh_l2'\n    )\n\n    email_failure = EmailOperator(\n        task_id='email_on_failure',\n        to=['b_alimov@ipakyulibank.uz'],\n        subject='Airflow DAG Execution Failure',\n        html_content=f'Your Airflow {dag.dag_id} encountered a failure.',\n        trigger_rule='one_failed',\n        dag=dag\n    )\n\n    with TaskGroup(group_id='segmentation_group') as segmentation_group:\n        load_ds_client_segmentation = TriggerDagRunOperator(\n            task_id='load_ds_client_segmentation',\n            trigger_dag_id='l1_to_l2_p_load_data_ds_client_segmentation_full',\n            reset_dag_run=True,\n            wait_for_completion=True,\n            execution_date='{{ dag_run.logical_date }}')\n        send_flg_to_sap = PythonOperator(\n            task_id='send_flg_to_sap',\n            python_callable=p_send_flg_to_sap,\n            dag=dag)\n\n        load_ds_client_segmentation >> send_flg_to_sap\n\n\n    wait_for_l2_full_load >> get_load_id >> workflow_registration >> sensor_success_end >> run_sys_kill_all_session_pg >> \\\n       [run_wf_data_preparation_for_reports,segmentation_group] >> end_task >> email_failure\n"
    }
  ],
  "sampled_time": "2025-08-22T20:28:21.750473Z",
  "analysis": {
    "is_valid_dag_file": true,
    "is_production_dag": true,
    "is_airflow_2": true,
    "processing_type": "batch",
    "has_streaming_operators": false,
    "has_ml_operators": false,
    "topology": {
      "pattern": "mixed",
      "has_sensors": true,
      "has_branches": false,
      "has_fan_out": true,
      "has_fan_in": true,
      "estimated_max_parallel_width": 2,
      "estimated_branch_depth": 0,
      "has_cycles": false,
      "has_subdags": false,
      "has_task_groups": true
    },
    "tasks": {
      "total_count": 10,
      "operator_types": [
        "PythonOperator",
        "SqlSensor",
        "ExternalTaskSensor",
        "TriggerDagRunOperator",
        "EmailOperator"
      ],
      "has_dynamic_mapping": false,
      "has_external_task_sensor": true
    },
    "description": "Main workflow for loading datasets from DWH L2 to L2 with segmentation processing and SAP integration",
    "complexity_score": 6
  }
}