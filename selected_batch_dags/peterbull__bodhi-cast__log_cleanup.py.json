{
  "unique_key": "peterbull/bodhi-cast:airflow/dags/log_cleanup.py",
  "repo": "peterbull/bodhi-cast",
  "repo_url": "https://github.com/peterbull/bodhi-cast",
  "stars": 2,
  "license": "Apache-2.0",
  "file_path": "airflow/dags/log_cleanup.py",
  "detected_airflow_version": "unknown",
  "sampled_versions": [
    {
      "version": "latest",
      "commit": "a199ee241d8a68391c7ee4aa4ef56c3261ca139a",
      "code": "\"\"\"\nA maintenance workflow that you can deploy into Airflow to periodically clean\nout the task logs to avoid those getting too big.\nairflow trigger_dag --conf '[curly-braces]\"maxLogAgeInDays\":30[curly-braces]' airflow-log-cleanup\n--conf options:\n    maxLogAgeInDays:<INT> - Optional\n\"\"\"\n\nimport logging\nimport os\nfrom datetime import timedelta\n\nimport jinja2\nfrom airflow.configuration import conf\nfrom airflow.models import DAG, Variable\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\n\nimport airflow\n\n# airflow-log-cleanup\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\nSTART_DATE = airflow.utils.dates.days_ago(1)\ntry:\n    BASE_LOG_FOLDER = conf.get(\"core\", \"BASE_LOG_FOLDER\").rstrip(\"/\")\nexcept Exception as e:\n    BASE_LOG_FOLDER = conf.get(\"logging\", \"BASE_LOG_FOLDER\").rstrip(\"/\")\n# How often to Run. @daily - Once a day at Midnight\nSCHEDULE_INTERVAL = \"@daily\"\n# Who is listed as the owner of this DAG in the Airflow Web Server\nDAG_OWNER_NAME = \"operations\"\n# List of email address to send email alerts to if this job fails\nALERT_EMAIL_ADDRESSES = []\n# Length to retain the log files if not already provided in the conf. If this\n# is set to 30, the job will remove those files that are 30 days old or older\nDEFAULT_MAX_LOG_AGE_IN_DAYS = Variable.get(\"airflow_log_cleanup__max_log_age_in_days\", 7)\n# Whether the job should delete the logs or not. Included if you want to\n# temporarily avoid deleting the logs\nENABLE_DELETE = True\n# The number of worker nodes you have in Airflow. Will attempt to run this\n# process for however many workers there are so that each worker gets its\n# logs cleared.\nNUMBER_OF_WORKERS = 1\nDIRECTORIES_TO_DELETE = [BASE_LOG_FOLDER]\nENABLE_DELETE_CHILD_LOG = Variable.get(\"airflow_log_cleanup__enable_delete_child_log\", \"False\")\nLOG_CLEANUP_PROCESS_LOCK_FILE = \"/tmp/airflow_log_cleanup_worker.lock\"\nlogging.info(\"ENABLE_DELETE_CHILD_LOG  \" + ENABLE_DELETE_CHILD_LOG)\n\nif not BASE_LOG_FOLDER or BASE_LOG_FOLDER.strip() == \"\":\n    raise ValueError(\n        \"BASE_LOG_FOLDER variable is empty in airflow.cfg. It can be found \"\n        \"under the [core] (<2.0.0) section or [logging] (>=2.0.0) in the cfg file. \"\n        \"Kindly provide an appropriate directory path.\"\n    )\n\nif ENABLE_DELETE_CHILD_LOG.lower() == \"true\":\n    try:\n        CHILD_PROCESS_LOG_DIRECTORY = conf.get(\"scheduler\", \"CHILD_PROCESS_LOG_DIRECTORY\")\n        if CHILD_PROCESS_LOG_DIRECTORY != \" \":\n            DIRECTORIES_TO_DELETE.append(CHILD_PROCESS_LOG_DIRECTORY)\n    except Exception as e:\n        logging.exception(\n            \"Could not obtain CHILD_PROCESS_LOG_DIRECTORY from \"\n            + \"Airflow Configurations: \"\n            + str(e)\n        )\n\ndefault_args = {\n    \"owner\": DAG_OWNER_NAME,\n    \"depends_on_past\": False,\n    \"email\": ALERT_EMAIL_ADDRESSES,\n    \"email_on_failure\": True,\n    \"email_on_retry\": False,\n    \"start_date\": START_DATE,\n    \"retries\": 1,\n    \"retry_delay\": timedelta(minutes=1),\n}\n\ndag = DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule_interval=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    tags=[\"teamclairvoyant\", \"airflow-maintenance-dags\"],\n    template_undefined=jinja2.Undefined,\n    is_paused_upon_creation=False,\n)\nif hasattr(dag, \"doc_md\"):\n    dag.doc_md = __doc__\nif hasattr(dag, \"catchup\"):\n    dag.catchup = False\n\nstart = EmptyOperator(task_id=\"start\", dag=dag)\n\nlog_cleanup = (\n    \"\"\"\n\necho \"Getting Configurations...\"\nBASE_LOG_FOLDER=\"{{params.directory}}\"\nWORKER_SLEEP_TIME=\"{{params.sleep_time}}\"\n\nsleep ${WORKER_SLEEP_TIME}s\n\nMAX_LOG_AGE_IN_DAYS=\"{{dag_run.conf.maxLogAgeInDays}}\"\nif [ \"${MAX_LOG_AGE_IN_DAYS}\" == \"\" ]; then\n    echo \"maxLogAgeInDays conf variable isn't included. Using Default '\"\"\"\n    + str(DEFAULT_MAX_LOG_AGE_IN_DAYS)\n    + \"\"\"'.\"\n    MAX_LOG_AGE_IN_DAYS='\"\"\"\n    + str(DEFAULT_MAX_LOG_AGE_IN_DAYS)\n    + \"\"\"'\nfi\nENABLE_DELETE=\"\"\"\n    + str(\"true\" if ENABLE_DELETE else \"false\")\n    + \"\"\"\necho \"Finished Getting Configurations\"\necho \"\"\n\necho \"Configurations:\"\necho \"BASE_LOG_FOLDER:      '${BASE_LOG_FOLDER}'\"\necho \"MAX_LOG_AGE_IN_DAYS:  '${MAX_LOG_AGE_IN_DAYS}'\"\necho \"ENABLE_DELETE:        '${ENABLE_DELETE}'\"\n\ncleanup() {\n    echo \"Executing Find Statement: $1\"\n    FILES_MARKED_FOR_DELETE=`eval $1`\n    echo \"Process will be Deleting the following File(s)/Directory(s):\"\n    echo \"${FILES_MARKED_FOR_DELETE}\"\n    echo \"Process will be Deleting `echo \"${FILES_MARKED_FOR_DELETE}\" | \\\n    grep -v '^$' | wc -l` File(s)/Directory(s)\"     \\\n    # \"grep -v '^$'\" - removes empty lines.\n    # \"wc -l\" - Counts the number of lines\n    echo \"\"\n    if [ \"${ENABLE_DELETE}\" == \"true\" ];\n    then\n        if [ \"${FILES_MARKED_FOR_DELETE}\" != \"\" ];\n        then\n            echo \"Executing Delete Statement: $2\"\n            eval $2\n            DELETE_STMT_EXIT_CODE=$?\n            if [ \"${DELETE_STMT_EXIT_CODE}\" != \"0\" ]; then\n                echo \"Delete process failed with exit code \\\n                    '${DELETE_STMT_EXIT_CODE}'\"\n\n                echo \"Removing lock file...\"\n                rm -f \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\"\n                if [ \"${REMOVE_LOCK_FILE_EXIT_CODE}\" != \"0\" ]; then\n                    echo \"Error removing the lock file. \\\n                    Check file permissions.\\\n                    To re-run the DAG, ensure that the lock file has been \\\n                    deleted (\"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\").\"\n                    exit ${REMOVE_LOCK_FILE_EXIT_CODE}\n                fi\n                exit ${DELETE_STMT_EXIT_CODE}\n            fi\n        else\n            echo \"WARN: No File(s)/Directory(s) to Delete\"\n        fi\n    else\n        echo \"WARN: You're opted to skip deleting the File(s)/Directory(s)!!!\"\n    fi\n}\n\n\nif [ ! -f \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\" ]; then\n\n    echo \"Lock file not found on this node! \\\n    Creating it to prevent collisions...\"\n    touch \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\"\n    CREATE_LOCK_FILE_EXIT_CODE=$?\n    if [ \"${CREATE_LOCK_FILE_EXIT_CODE}\" != \"0\" ]; then\n        echo \"Error creating the lock file. \\\n        Check if the airflow user can create files under tmp directory. \\\n        Exiting...\"\n        exit ${CREATE_LOCK_FILE_EXIT_CODE}\n    fi\n\n    echo \"\"\n    echo \"Running Cleanup Process...\"\n\n    FIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/*/* -type f -mtime \\\n     +${MAX_LOG_AGE_IN_DAYS}\"\n    DELETE_STMT=\"${FIND_STATEMENT} -exec rm -f {} \\;\"\n\n    cleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\n    CLEANUP_EXIT_CODE=$?\n\n    FIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/*/* -type d -empty\"\n    DELETE_STMT=\"${FIND_STATEMENT} -prune -exec rm -rf {} \\;\"\n\n    cleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\n    CLEANUP_EXIT_CODE=$?\n\n    FIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/* -type d -empty\"\n    DELETE_STMT=\"${FIND_STATEMENT} -prune -exec rm -rf {} \\;\"\n\n    cleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\n    CLEANUP_EXIT_CODE=$?\n\n    echo \"Finished Running Cleanup Process\"\n\n    echo \"Deleting lock file...\"\n    rm -f \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\"\n    REMOVE_LOCK_FILE_EXIT_CODE=$?\n    if [ \"${REMOVE_LOCK_FILE_EXIT_CODE}\" != \"0\" ]; then\n        echo \"Error removing the lock file. Check file permissions. To re-run the DAG, ensure that the lock file has been deleted (\"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\").\"\n        exit ${REMOVE_LOCK_FILE_EXIT_CODE}\n    fi\n\nelse\n    echo \"Another task is already deleting logs on this worker node. \\\n    Skipping it!\"\n    echo \"If you believe you're receiving this message in error, kindly check \\\n    if \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\" exists and delete it.\"\n    exit 0\nfi\n\n\"\"\"\n)\n\nfor log_cleanup_id in range(1, NUMBER_OF_WORKERS + 1):\n\n    for dir_id, directory in enumerate(DIRECTORIES_TO_DELETE):\n\n        log_cleanup_op = BashOperator(\n            task_id=\"log_cleanup_worker_num_\" + str(log_cleanup_id) + \"_dir_\" + str(dir_id),\n            bash_command=log_cleanup,\n            params={\"directory\": str(directory), \"sleep_time\": int(log_cleanup_id) * 3},\n            dag=dag,\n        )\n\n        log_cleanup_op.set_upstream(start)\n"
    },
    {
      "version": "earliest",
      "commit": "edc0b227ccecfa53d91c2f5a4fdc44478e2b7168",
      "code": "\"\"\"\nA maintenance workflow that you can deploy into Airflow to periodically clean\nout the task logs to avoid those getting too big.\nairflow trigger_dag --conf '[curly-braces]\"maxLogAgeInDays\":30[curly-braces]' airflow-log-cleanup\n--conf options:\n    maxLogAgeInDays:<INT> - Optional\n\"\"\"\n\nimport logging\nimport os\nfrom datetime import timedelta\n\nimport jinja2\nfrom airflow.configuration import conf\nfrom airflow.models import DAG, Variable\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.empty import EmptyOperator\n\nimport airflow\n\n# airflow-log-cleanup\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\nSTART_DATE = airflow.utils.dates.days_ago(1)\ntry:\n    BASE_LOG_FOLDER = conf.get(\"core\", \"BASE_LOG_FOLDER\").rstrip(\"/\")\nexcept Exception as e:\n    BASE_LOG_FOLDER = conf.get(\"logging\", \"BASE_LOG_FOLDER\").rstrip(\"/\")\n# How often to Run. @daily - Once a day at Midnight\nSCHEDULE_INTERVAL = \"@daily\"\n# Who is listed as the owner of this DAG in the Airflow Web Server\nDAG_OWNER_NAME = \"operations\"\n# List of email address to send email alerts to if this job fails\nALERT_EMAIL_ADDRESSES = []\n# Length to retain the log files if not already provided in the conf. If this\n# is set to 30, the job will remove those files that are 30 days old or older\nDEFAULT_MAX_LOG_AGE_IN_DAYS = Variable.get(\"airflow_log_cleanup__max_log_age_in_days\", 7)\n# Whether the job should delete the logs or not. Included if you want to\n# temporarily avoid deleting the logs\nENABLE_DELETE = True\n# The number of worker nodes you have in Airflow. Will attempt to run this\n# process for however many workers there are so that each worker gets its\n# logs cleared.\nNUMBER_OF_WORKERS = 1\nDIRECTORIES_TO_DELETE = [BASE_LOG_FOLDER]\nENABLE_DELETE_CHILD_LOG = Variable.get(\"airflow_log_cleanup__enable_delete_child_log\", \"False\")\nLOG_CLEANUP_PROCESS_LOCK_FILE = \"/tmp/airflow_log_cleanup_worker.lock\"\nlogging.info(\"ENABLE_DELETE_CHILD_LOG  \" + ENABLE_DELETE_CHILD_LOG)\n\nif not BASE_LOG_FOLDER or BASE_LOG_FOLDER.strip() == \"\":\n    raise ValueError(\n        \"BASE_LOG_FOLDER variable is empty in airflow.cfg. It can be found \"\n        \"under the [core] (<2.0.0) section or [logging] (>=2.0.0) in the cfg file. \"\n        \"Kindly provide an appropriate directory path.\"\n    )\n\nif ENABLE_DELETE_CHILD_LOG.lower() == \"true\":\n    try:\n        CHILD_PROCESS_LOG_DIRECTORY = conf.get(\"scheduler\", \"CHILD_PROCESS_LOG_DIRECTORY\")\n        if CHILD_PROCESS_LOG_DIRECTORY != \" \":\n            DIRECTORIES_TO_DELETE.append(CHILD_PROCESS_LOG_DIRECTORY)\n    except Exception as e:\n        logging.exception(\n            \"Could not obtain CHILD_PROCESS_LOG_DIRECTORY from \"\n            + \"Airflow Configurations: \"\n            + str(e)\n        )\n\ndefault_args = {\n    \"owner\": DAG_OWNER_NAME,\n    \"depends_on_past\": False,\n    \"email\": ALERT_EMAIL_ADDRESSES,\n    \"email_on_failure\": True,\n    \"email_on_retry\": False,\n    \"start_date\": START_DATE,\n    \"retries\": 1,\n    \"retry_delay\": timedelta(minutes=1),\n}\n\ndag = DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule_interval=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    tags=[\"teamclairvoyant\", \"airflow-maintenance-dags\"],\n    template_undefined=jinja2.Undefined,\n)\nif hasattr(dag, \"doc_md\"):\n    dag.doc_md = __doc__\nif hasattr(dag, \"catchup\"):\n    dag.catchup = False\n\nstart = EmptyOperator(task_id=\"start\", dag=dag)\n\nlog_cleanup = (\n    \"\"\"\n\necho \"Getting Configurations...\"\nBASE_LOG_FOLDER=\"{{params.directory}}\"\nWORKER_SLEEP_TIME=\"{{params.sleep_time}}\"\n\nsleep ${WORKER_SLEEP_TIME}s\n\nMAX_LOG_AGE_IN_DAYS=\"{{dag_run.conf.maxLogAgeInDays}}\"\nif [ \"${MAX_LOG_AGE_IN_DAYS}\" == \"\" ]; then\n    echo \"maxLogAgeInDays conf variable isn't included. Using Default '\"\"\"\n    + str(DEFAULT_MAX_LOG_AGE_IN_DAYS)\n    + \"\"\"'.\"\n    MAX_LOG_AGE_IN_DAYS='\"\"\"\n    + str(DEFAULT_MAX_LOG_AGE_IN_DAYS)\n    + \"\"\"'\nfi\nENABLE_DELETE=\"\"\"\n    + str(\"true\" if ENABLE_DELETE else \"false\")\n    + \"\"\"\necho \"Finished Getting Configurations\"\necho \"\"\n\necho \"Configurations:\"\necho \"BASE_LOG_FOLDER:      '${BASE_LOG_FOLDER}'\"\necho \"MAX_LOG_AGE_IN_DAYS:  '${MAX_LOG_AGE_IN_DAYS}'\"\necho \"ENABLE_DELETE:        '${ENABLE_DELETE}'\"\n\ncleanup() {\n    echo \"Executing Find Statement: $1\"\n    FILES_MARKED_FOR_DELETE=`eval $1`\n    echo \"Process will be Deleting the following File(s)/Directory(s):\"\n    echo \"${FILES_MARKED_FOR_DELETE}\"\n    echo \"Process will be Deleting `echo \"${FILES_MARKED_FOR_DELETE}\" | \\\n    grep -v '^$' | wc -l` File(s)/Directory(s)\"     \\\n    # \"grep -v '^$'\" - removes empty lines.\n    # \"wc -l\" - Counts the number of lines\n    echo \"\"\n    if [ \"${ENABLE_DELETE}\" == \"true\" ];\n    then\n        if [ \"${FILES_MARKED_FOR_DELETE}\" != \"\" ];\n        then\n            echo \"Executing Delete Statement: $2\"\n            eval $2\n            DELETE_STMT_EXIT_CODE=$?\n            if [ \"${DELETE_STMT_EXIT_CODE}\" != \"0\" ]; then\n                echo \"Delete process failed with exit code \\\n                    '${DELETE_STMT_EXIT_CODE}'\"\n\n                echo \"Removing lock file...\"\n                rm -f \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\"\n                if [ \"${REMOVE_LOCK_FILE_EXIT_CODE}\" != \"0\" ]; then\n                    echo \"Error removing the lock file. \\\n                    Check file permissions.\\\n                    To re-run the DAG, ensure that the lock file has been \\\n                    deleted (\"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\").\"\n                    exit ${REMOVE_LOCK_FILE_EXIT_CODE}\n                fi\n                exit ${DELETE_STMT_EXIT_CODE}\n            fi\n        else\n            echo \"WARN: No File(s)/Directory(s) to Delete\"\n        fi\n    else\n        echo \"WARN: You're opted to skip deleting the File(s)/Directory(s)!!!\"\n    fi\n}\n\n\nif [ ! -f \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\" ]; then\n\n    echo \"Lock file not found on this node! \\\n    Creating it to prevent collisions...\"\n    touch \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\"\n    CREATE_LOCK_FILE_EXIT_CODE=$?\n    if [ \"${CREATE_LOCK_FILE_EXIT_CODE}\" != \"0\" ]; then\n        echo \"Error creating the lock file. \\\n        Check if the airflow user can create files under tmp directory. \\\n        Exiting...\"\n        exit ${CREATE_LOCK_FILE_EXIT_CODE}\n    fi\n\n    echo \"\"\n    echo \"Running Cleanup Process...\"\n\n    FIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/*/* -type f -mtime \\\n     +${MAX_LOG_AGE_IN_DAYS}\"\n    DELETE_STMT=\"${FIND_STATEMENT} -exec rm -f {} \\;\"\n\n    cleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\n    CLEANUP_EXIT_CODE=$?\n\n    FIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/*/* -type d -empty\"\n    DELETE_STMT=\"${FIND_STATEMENT} -prune -exec rm -rf {} \\;\"\n\n    cleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\n    CLEANUP_EXIT_CODE=$?\n\n    FIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/* -type d -empty\"\n    DELETE_STMT=\"${FIND_STATEMENT} -prune -exec rm -rf {} \\;\"\n\n    cleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\n    CLEANUP_EXIT_CODE=$?\n\n    echo \"Finished Running Cleanup Process\"\n\n    echo \"Deleting lock file...\"\n    rm -f \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\"\n    REMOVE_LOCK_FILE_EXIT_CODE=$?\n    if [ \"${REMOVE_LOCK_FILE_EXIT_CODE}\" != \"0\" ]; then\n        echo \"Error removing the lock file. Check file permissions. To re-run the DAG, ensure that the lock file has been deleted (\"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\").\"\n        exit ${REMOVE_LOCK_FILE_EXIT_CODE}\n    fi\n\nelse\n    echo \"Another task is already deleting logs on this worker node. \\\n    Skipping it!\"\n    echo \"If you believe you're receiving this message in error, kindly check \\\n    if \"\"\"\n    + str(LOG_CLEANUP_PROCESS_LOCK_FILE)\n    + \"\"\" exists and delete it.\"\n    exit 0\nfi\n\n\"\"\"\n)\n\nfor log_cleanup_id in range(1, NUMBER_OF_WORKERS + 1):\n\n    for dir_id, directory in enumerate(DIRECTORIES_TO_DELETE):\n\n        log_cleanup_op = BashOperator(\n            task_id=\"log_cleanup_worker_num_\" + str(log_cleanup_id) + \"_dir_\" + str(dir_id),\n            bash_command=log_cleanup,\n            params={\"directory\": str(directory), \"sleep_time\": int(log_cleanup_id) * 3},\n            dag=dag,\n        )\n\n        log_cleanup_op.set_upstream(start)\n"
    }
  ],
  "sampled_time": "2025-11-24T22:04:35.347589Z",
  "analysis": {
    "is_valid_dag_file": true,
    "is_production_dag": true,
    "is_airflow_2": true,
    "processing_type": "batch",
    "has_streaming_operators": false,
    "has_ml_operators": false,
    "topology": {
      "pattern": "fan_out_fan_in",
      "has_sensors": false,
      "has_branches": false,
      "has_fan_out": true,
      "has_fan_in": false,
      "estimated_max_parallel_width": 2,
      "estimated_branch_depth": 0,
      "has_cycles": false,
      "has_subdags": false,
      "has_task_groups": false
    },
    "tasks": {
      "total_count": 3,
      "operator_types": [
        "EmptyOperator",
        "BashOperator"
      ],
      "has_dynamic_mapping": false,
      "has_external_task_sensor": false
    },
    "description": "Airflow log cleanup DAG that periodically removes old log files using BashOperator tasks with parallel execution for multiple workers",
    "complexity_score": 3
  }
}