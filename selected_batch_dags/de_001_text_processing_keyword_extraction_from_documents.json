{
    "unique_key": "text_processing:keyword_extraction_dag.py",
    "repo": "synthetic-data-engineering",
    "repo_url": "https://github.com/internal/data-processing-scripts",
    "stars": 0,
    "license": "MIT",
    "file_path": "dags/text_processing/keyword_extraction_dag.py",
    "detected_airflow_version": "2.x",
    "sampled_versions": [
      {
        "version": "1.0",
        "commit": "airflow_migration",
        "code": "import pandas as pd\nimport numpy as np\nimport random\nimport re\nimport json\nimport os\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List\nimport warnings\n\nfrom airflow import DAG\nfrom airflow.decorators import task\nfrom airflow.utils.dates import days_ago\n\n# Filter warnings for cleaner logs\nwarnings.filterwarnings('ignore')\n\n# Default arguments for the DAG\ndefault_args = {\n    'owner': 'data_engineering',\n    'depends_on_past': False,\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Define the DAG\n@DAG(\n    dag_id='keyword_extraction_pipeline',\n    default_args=default_args,\n    description='ETL pipeline for document keyword extraction and validation',\n    schedule_interval='@daily',\n    start_date=days_ago(1),\n    catchup=False,\n    tags=['text_processing', 'synthetic', 'etl']\n)\ndef keyword_extraction_dag():\n    \"\"\"\n    Airflow DAG Implementation of the Keyword Extraction Pipeline.\n    \n    Flow:\n    1. Generate synthetic document data\n    2. Clean and standardize data\n    3. Extract features (keyword density, complexity)\n    4. Validate data against business rules\n    5. Archive results and log summary\n    \"\"\"\n\n    # Shared temp directory for intermediate files (simulating a shared volume)\n    TEMP_DIR = '/tmp/airflow_processing'\n    os.makedirs(TEMP_DIR, exist_ok=True)\n\n    @task\n    def generate_documents() -> str:\n        \"\"\"\n        Generate synthetic document data and save to temporary parquet file.\n        Returns the file path.\n        \"\"\"\n        print(\"Generating synthetic documents...\")\n        num_rows = 3500\n        \n        # Set seeds\n        random.seed(42)\n        np.random.seed(42)\n        \n        # Categories and Keywords\n        categories = ['Technical', 'Business', 'Scientific', 'Legal', 'Medical', \n                      'Educational', 'Marketing', 'News', 'Research', 'Government']\n        \n        category_keywords = {\n            'Technical': ['python', 'algorithm', 'database', 'server', 'cloud', 'api', 'security'],\n            'Business': ['revenue', 'strategy', 'market', 'growth', 'investment', 'profit', 'leadership'],\n            'Scientific': ['experiment', 'hypothesis', 'data', 'analysis', 'results', 'methodology', 'conclusion'],\n            'Legal': ['contract', 'agreement', 'liability', 'compliance', 'regulation', 'clause', 'jurisdiction'],\n            'Medical': ['patient', 'treatment', 'diagnosis', 'symptoms', 'medication', 'therapy', 'recovery'],\n            'Educational': ['learning', 'curriculum', 'assessment', 'student', 'teaching', 'knowledge', 'skills'],\n            'Marketing': ['campaign', 'brand', 'audience', 'conversion', 'engagement', 'ROI', 'strategy'],\n            'News': ['breaking', 'update', 'report', 'source', 'coverage', 'developing', 'exclusive'],\n            'Research': ['study', 'findings', 'publication', 'peer-reviewed', 'methodology', 'limitations', 'future'],\n            'Government': ['policy', 'regulation', 'public', 'service', 'administration', 'funding', 'initiative']\n        }\n        \n        # Generation Logic\n        doc_ids = [f'DOC-{i:06d}' for i in range(1, num_rows + 1)]\n        \n        dates = []\n        for i in range(num_rows):\n            if i % 10 == 0: days_ago_val = random.randint(0, 7)\n            elif i % 5 == 0: days_ago_val = random.randint(8, 30)\n            else: days_ago_val = random.randint(31, 365)\n            dates.append(datetime.now() - timedelta(days=days_ago_val))\n        \n        categories_list = []\n        for i in range(num_rows):\n            if random.random() < 0.05: categories_list.append(None)\n            else: categories_list.append(random.choice(categories))\n        \n        lengths = np.random.lognormal(mean=7, sigma=1.2, size=num_rows).astype(int)\n        lengths = np.clip(lengths, 100, 10000)\n        \n        readability = np.random.normal(loc=60, scale=20, size=num_rows)\n        readability = np.clip(readability, 0, 100)\n        \n        sentiment = np.random.normal(loc=0.2, scale=0.5, size=num_rows)\n        sentiment = np.clip(sentiment, -1, 1)\n        \n        keyword_counts = np.random.poisson(lam=8, size=num_rows)\n        outlier_indices = random.sample(range(num_rows), int(num_rows * 0.015))\n        for idx in outlier_indices: keyword_counts[idx] = random.randint(50, 100)\n        \n        authors = []\n        first_names = ['John', 'Jane', 'Robert', 'Mary', 'Michael', 'Sarah', 'David', 'Lisa']\n        last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Miller', 'Davis', 'Wilson']\n        for i in range(num_rows):\n            if random.random() < 0.08: authors.append(None)\n            else:\n                format_choice = random.random()\n                if format_choice < 0.3: authors.append(f\"{random.choice(last_names)}, {random.choice(first_names)}\")\n                elif format_choice < 0.6: authors.append(f\"{random.choice(first_names)} {random.choice(last_names)}\")\n                else: authors.append(f\"{random.choice(first_names)[0]}. {random.choice(last_names)}\")\n\n        statuses = ['Published', 'Draft', 'Review', 'Archived', 'Deleted']\n        status_list = []\n        for i in range(num_rows):\n            if random.random() < 0.02: status_list.append('Invalid_Status')\n            else: status_list.append(random.choice(statuses))\n\n        tags_list = []\n        for i in range(num_rows):\n            num_tags = random.randint(0, 5)\n            tags = []\n            for _ in range(num_tags):\n                if random.random() < 0.05: tags.append(f\"tag with spaces {random.randint(1, 100)}\")\n                else: tags.append(f\"tag{random.randint(1, 20)}\")\n            tags_list.append(\", \".join(tags))\n\n        df = pd.DataFrame({\n            'document_id': doc_ids,\n            'creation_date': dates,\n            'category': categories_list,\n            'word_count': lengths,\n            'readability_score': readability,\n            'sentiment_score': sentiment,\n            'keyword_count': keyword_counts,\n            'author': authors,\n            'publication_status': status_list,\n            'tags': tags_list,\n            'version': np.random.choice(['1.0', '2.0', '3.0', 'draft'], size=num_rows, p=[0.4, 0.3, 0.2, 0.1])\n        })\n        \n        # Add duplicates\n        duplicate_indices = random.sample(range(num_rows), int(num_rows * 0.01))\n        for idx in duplicate_indices:\n            if idx + 1 < num_rows: df.iloc[idx + 1] = df.iloc[idx].copy()\n\n        output_path = os.path.join(TEMP_DIR, 'raw_documents.parquet')\n        df.to_parquet(output_path, index=False)\n        print(f\"Generated {len(df)} documents. Saved to {output_path}\")\n        return output_path\n\n    @task\n    def explore_data(file_path: str):\n        \"\"\"Log exploration stats of the raw data.\"\"\"\n        df = pd.read_parquet(file_path)\n        print(\"DATA EXPLORATION REPORT\")\n        print(f\"Shape: {df.shape}\")\n        print(f\"Missing Values:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")\n        print(f\"Categories: {df['category'].nunique()}\")\n\n    @task\n    def clean_data(file_path: str) -> str:\n        \"\"\"Clean the raw data and save.\"\"\"\n        print(\"Cleaning data...\")\n        df = pd.read_parquet(file_path)\n        \n        # 1. Fill missing\n        df['category'] = df['category'].fillna('Unknown')\n        df['author'] = df['author'].fillna('Anonymous')\n        \n        # 2. Standardize authors\n        def standardize_author(name):\n            if name == 'Anonymous': return name\n            name = ' '.join(name.split())\n            if ',' in name:\n                parts = name.split(',')\n                if len(parts) >= 2: return f\"{parts[1].strip()} {parts[0].strip()}\"\n            return name\n        df['author_standardized'] = df['author'].apply(standardize_author)\n        \n        # 3. Handle outliers\n        keyword_q99 = df['keyword_count'].quantile(0.99)\n        df['keyword_count_clean'] = df['keyword_count'].clip(upper=keyword_q99)\n        \n        # 4. Fix status\n        valid_statuses = ['Published', 'Draft', 'Review', 'Archived', 'Deleted']\n        df.loc[~df['publication_status'].isin(valid_statuses), 'publication_status'] = 'Draft'\n        \n        # 5. Clean tags\n        def clean_tags(tag_string):\n            if not tag_string: return ''\n            tags = [tag.strip() for tag in tag_string.split(',')]\n            cleaned = [re.sub(r'[^a-zA-Z0-9_]', '_', tag).lower() for tag in tags if tag.strip()]\n            return ', '.join(cleaned)\n        df['tags_clean'] = df['tags'].apply(clean_tags)\n        \n        # 6. Deduplicate\n        df = df.drop_duplicates(subset=['document_id'], keep='first')\n        \n        output_path = os.path.join(TEMP_DIR, 'cleaned_documents.parquet')\n        df.to_parquet(output_path, index=False)\n        print(f\"Data cleaned. Rows: {len(df)}. Saved to {output_path}\")\n        return output_path\n\n    @task\n    def extract_features(file_path: str) -> str:\n        \"\"\"Extract keyword features and metrics.\"\"\"\n        print(\"Extracting features...\")\n        df = pd.read_parquet(file_path)\n        \n        df['keyword_density'] = (df['keyword_count_clean'] / df['word_count']) * 100\n        \n        df['complexity_score'] = (\n            (100 - df['readability_score']) * 0.6 + \n            df['keyword_density'] * 0.4\n        ) / 100\n        \n        def categorize_length(wc): \n            return 'Short' if wc < 500 else 'Medium' if wc < 2000 else 'Long'\n        df['length_category'] = df['word_count'].apply(categorize_length)\n        \n        current_date = datetime.now()\n        df['days_since_creation'] = df['creation_date'].apply(lambda x: (current_date - x).days)\n        \n        df['tag_count'] = df['tags_clean'].apply(lambda x: len(x.split(', ')) if x else 0)\n        \n        df['quality_score'] = (\n            df['readability_score'] * 0.3 +\n            (1 - abs(df['sentiment_score'])) * 0.2 +\n            np.log1p(df['word_count']) * 0.2 +\n            (df['keyword_count_clean'] / 20) * 0.3\n        )\n        \n        output_path = os.path.join(TEMP_DIR, 'processed_documents.parquet')\n        df.to_parquet(output_path, index=False)\n        return output_path\n\n    @task\n    def validate_data(file_path: str) -> Dict:\n        \"\"\"Validate processed data and return result dict.\"\"\"\n        print(\"Validating data...\")\n        df = pd.read_parquet(file_path)\n        \n        results = {'total_documents': len(df), 'checks': {}}\n        \n        # Checks\n        results['checks']['missing_ids'] = int(df['document_id'].isnull().sum())\n        results['checks']['invalid_cats'] = int((~df['category'].isin(['Technical', 'Business', 'Scientific', 'Legal', 'Medical', 'Educational', 'Marketing', 'News', 'Research', 'Government', 'Unknown'])).sum())\n        results['checks']['word_count_ok'] = bool(df['word_count'].between(10, 20000).all())\n        results['checks']['future_dates'] = int((df['creation_date'] > datetime.now()).sum())\n        \n        issues = results['checks']['missing_ids'] + results['checks']['invalid_cats'] + results['checks']['future_dates']\n        if not results['checks']['word_count_ok']: issues += 1\n        \n        results['status'] = 'PASSED' if issues == 0 else 'FAILED'\n        results['issues_count'] = issues\n        \n        print(f\"Validation completed. Status: {results['status']}\")\n        return results\n\n    @task\n    def save_and_report(file_path: str, validation_results: Dict):\n        \"\"\"Save final artifacts and log report.\"\"\"\n        print(\"Saving results...\")\n        df = pd.read_parquet(file_path)\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        \n        # In a real scenario, this would upload to S3/GCS\n        final_csv = os.path.join(TEMP_DIR, f\"final_output_{timestamp}.csv\")\n        df.to_csv(final_csv, index=False)\n        \n        print(\"=\"*60)\n        print(\"FINAL PIPELINE REPORT\")\n        print(\"=\"*60)\n        print(f\"Final Dataset: {len(df)} rows\")\n        print(f\"Validation Status: {validation_results['status']}\")\n        print(f\"Issues Found: {validation_results['issues_count']}\")\n        print(f\"Output Saved To: {final_csv}\")\n        \n        if validation_results['status'] == 'FAILED':\n            print(\"WARNING: Data validation failed. Please check logs.\")\n\n    # Task Orchestration\n    raw_file = generate_documents()\n    explore_data(raw_file)\n    cleaned_file = clean_data(raw_file)\n    processed_file = extract_features(cleaned_file)\n    validation_res = validate_data(processed_file)\n    save_and_report(processed_file, validation_res)\n\n# Instantiate DAG\ndag_instance = keyword_extraction_dag()"
      }
    ],
    "sampled_time": "2025-12-15T21:35:00Z",
    "analysis": {
      "is_valid_dag_file": true,
      "is_production_dag": true,
      "is_airflow_2": true,
      "processing_type": "batch",
      "has_streaming_operators": false,
      "has_ml_operators": false,
      "topology": {
        "pattern": "linear",
        "has_sensors": false,
        "has_branches": false,
        "has_fan_out": false,
        "has_fan_in": false,
        "estimated_max_parallel_width": 1,
        "estimated_branch_depth": 0,
        "has_cycles": false,
        "has_subdags": false,
        "has_task_groups": false
      },
      "tasks": {
        "total_count": 6,
        "operator_types": [
          "PythonOperator (TaskFlow)"
        ],
        "has_dynamic_mapping": false,
        "has_external_task_sensor": false
      },
      "description": "Airflow 2.x DAG implementing a text processing ETL pipeline. It generates synthetic documents, cleanses and standardizes them, calculates keyword density and complexity features, validates data integrity, and generates a final report. Data is passed between tasks using intermediate Parquet files.",
      "complexity_score": 3
    }
  }