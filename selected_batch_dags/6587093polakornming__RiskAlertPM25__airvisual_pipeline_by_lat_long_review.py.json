{
  "unique_key": "6587093polakornming/RiskAlertPM25:code_review/airvisual_pipeline_by_lat_long_review.py",
  "repo": "6587093polakornming/RiskAlertPM25",
  "repo_url": "https://github.com/6587093polakornming/RiskAlertPM25",
  "stars": 0,
  "license": "MIT",
  "file_path": "code_review/airvisual_pipeline_by_lat_long_review.py",
  "detected_airflow_version": "2.x",
  "sampled_versions": [
    {
      "version": "latest",
      "commit": "674e83d75e86dc799df2fb811d1cd7d40370359b",
      "code": "import json\nimport requests\nimport logging\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport configparser\n\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom airflow.exceptions import AirflowSkipException\n\n# ============ CONFIG ============ #\nBASE_DIR = Path(__file__).resolve().parent.parent\nCONFIG_PATH = BASE_DIR / \"config\" / \"config.conf\"\nOUTPUT_DIR = BASE_DIR / \"data\"\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nOUTPUT_FILE = OUTPUT_DIR / \"tmp_airvisual.json\"\n\nCITY = \"Salaya\"\nSTATE = \"Nakhon Pathom\"\nCOUNTRY = \"Thailand\"\nLATITUDE = 13.79059242\nLONGITUDE = 100.32622308\n\n# ============ FUNCTIONS ============ #\ndef get_cursor():\n    hook = PostgresHook(postgres_conn_id=\"postgres_conn\")\n    conn = hook.get_conn()\n    return conn, conn.cursor()\n\ndef is_data_in_db(date_time_str):\n    conn, cursor = get_cursor()\n    try:\n        cursor.execute(\"\"\"\n            SELECT 1 FROM factairvisualtable\n            WHERE date_time = %s\n            LIMIT 1\n        \"\"\", (date_time_str,))\n        return cursor.fetchone() is not None\n    finally:\n        cursor.close()\n        conn.close()\n\ndef get_data_from_airvisual():\n    config = configparser.ConfigParser()\n    config.read(CONFIG_PATH)\n\n    api_key = config.get(\"api\", \"airvisual_key\")\n\n    # URL = (\n    #     \"http://api.airvisual.com/v2/city\"\n    #     f\"?city={CITY}&state={STATE}&country={COUNTRY}&key={api_key}\"\n    # )\n    URL = (\n        \"http://api.airvisual.com/v2/nearest_city\"\n        f\"?lat={LATITUDE}&lon={LONGITUDE}&key={api_key}\"\n    )\n\n    logging.info(f\"Requesting data from: {URL}\")\n    try:\n        response = requests.get(URL, timeout=10)\n        response.raise_for_status()\n        data = response.json()\n\n        # ตรวจสอบ datetime ของข้อมูลล่าสุด\n        ts_str = data[\"data\"][\"current\"][\"pollution\"][\"ts\"]\n        ts_obj = datetime.strptime(ts_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n        ts_bangkok = ts_obj + timedelta(hours=7)\n        current_dt_str = ts_bangkok.strftime(\"%Y-%m-%d %H:%M\")\n\n        # เชื่อมต่อ Database แล้วตรวจสอบว่า datetime นี้มีอยู่แล้วหรือไม่\n        data_time_now_obj = datetime.now()+ timedelta(hours=7)\n        data_time_now_str = data_time_now_obj.strftime(\"%Y-%m-%d %H:00\") \n        if is_data_in_db(data_time_now_str):\n            raise AirflowSkipException(f\"Data for {data_time_now_str} already exists in DB. Skipping DAG.\")\n\n        # โหลด datetime ปัจจุบันจากไฟล์ JSON ที่มีอยู่แล้ว\n        if OUTPUT_FILE.exists():\n            with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f_check:\n                prev_data = json.load(f_check)\n            prev_dt_str = prev_data[\"data\"][\"current\"][\"pollution\"][\"ts\"]\n            prev_dt_obj = datetime.strptime(prev_dt_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n            prev_dt_obj = prev_dt_obj + timedelta(hours=7)\n            prev_dt_str = prev_dt_obj.strftime(\"%Y-%m-%d %H:%M\")\n            if current_dt_str == prev_dt_str and is_data_in_db(prev_dt_str):\n                raise Exception(f\"Data has not been updated yet: {current_dt_str} == {prev_dt_str}\")\n\n        # ใช้หลักการ \"Atomic File Write\":\n        # เขียนลงไฟล์ชั่วคราว (.tmp) แล้วค่อย rename เป็นไฟล์จริง\n        # เพื่อป้องกันปัญaหาการอ่านไฟล์ไม่สมบูรณ์ใน Task ถัดไป\n        tmp_path = Path(str(OUTPUT_FILE) + \".tmp\")\n        with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, ensure_ascii=False, indent=2)\n\n        tmp_path.rename(OUTPUT_FILE)\n        logging.info(\"Data saved successfully.\")\n\n    except requests.exceptions.RequestException as e:\n        logging.error(f\"Request failed: {e}\")\n        raise\n\ndef read_json_data():\n    try:\n        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        logging.info(\"Extracted data: %s\", data)\n        logging.info(\"Read JSON file successfully.\")\n    except Exception as e:\n        logging.error(f\"Error reading JSON file: {e}\")\n        raise\n\ndef load_airvisual_to_postgres():\n    \n    conn ,cursor = get_cursor() \n    try:\n        # Load pollution mapping from config\n        with open('/opt/airflow/config/mapping_main_pollution.json', 'r', encoding='utf-8') as mf:\n            pollution_mapping = json.load(mf)\n\n        # Load Weather code data\n        with open('/opt/airflow/config/mapping_weather_code.json', 'r', encoding='utf-8') as wf:\n            weather_mapping = json.load(wf)\n\n        # Load raw JSON data\n        with open('/opt/airflow/data/tmp_airvisual.json', 'r', encoding='utf-8') as f:\n            raw_data = json.load(f)\n        \n        date_time_str = raw_data[\"data\"][\"current\"][\"pollution\"][\"ts\"]\n        date_time_obj = datetime.strptime(date_time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n        date_time_obj_bangkok = date_time_obj + timedelta(hours=7)\n        date_str = date_time_obj_bangkok.strftime(\"%Y-%m-%d\")\n        time_str = date_time_obj_bangkok.strftime(\"%H:%M\")\n\n        # --- Step 1: Insert into dimDateTimeTable (if not exists) ---\n        cursor.execute(\"\"\"\n            INSERT INTO dimDateTimeTable (date_time, date, time, day, month, year, hour)\n            VALUES (%s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT (date_time) DO NOTHING;\n        \"\"\", (\n            date_time_obj_bangkok, \n            date_str,\n            time_str,\n            date_time_obj_bangkok.day,\n            date_time_obj_bangkok.month,\n            date_time_obj_bangkok.year,\n            date_time_obj_bangkok.hour\n        ))\n\n        # --- Step 2: Insert into dimLocationTable (if not exists) ---\n        contents_data: dict = raw_data[\"data\"]\n        latitude = round(float(contents_data[\"location\"][\"coordinates\"][1]), 6)\n        longitude = round(float(contents_data[\"location\"][\"coordinates\"][0]), 6)\n        country = contents_data[\"country\"]\n        state = contents_data[\"state\"]\n        city = contents_data[\"city\"]\n        description = f\"{city} {state} {country}\"\n\n        cursor.execute(\"\"\"\n            SELECT description, location_id FROM dimLocationTable\n            WHERE description = %s;\n        \"\"\", (description,))\n        result = cursor.fetchone()\n        print(f\"fecth cursor result find location id: {result}\")\n        if result:\n            location_id = result[1]\n        else:\n            cursor.execute(\"\"\" \n                INSERT INTO dimLocationTable (latitude, longitude, description, country, state, city)\n                VALUES (%s, %s, %s, %s, %s, %s)\n                RETURNING location_id;\n            \"\"\", (latitude, longitude, description, country, state, city))\n            location_id = cursor.fetchone()[0]\n\n         # --- Step 3: Insert into dimMainPollutionTable (if not exists) ---\n        main_code = raw_data[\"data\"][\"current\"][\"pollution\"][\"mainus\"]\n        if main_code not in pollution_mapping:\n            logging.warning(f\"Pollution code '{main_code}' not found in mapping file.\")\n        mapping = pollution_mapping.get(main_code, {\"unit\": \"unknown\", \"name_pollution\": main_code})\n        cursor.execute(\"\"\"\n            INSERT INTO dimMainPollutionTable (main_pollution_code, unit, name_pollution)\n            VALUES (%s, %s, %s)\n            ON CONFLICT (main_pollution_code) DO NOTHING;\n        \"\"\", (main_code, mapping['unit'], mapping['name_pollution']))\n\n        # --- Step 4: Insert into factAirVisualTable ---\n        weather_data: dict = contents_data.get(\"current\").get(\"weather\")\n        weather_code = weather_data.get(\"ic\")\n        if weather_code not in weather_mapping:\n            logging.warning(f\"weather code '{weather_code}' not found in mapping file.\")\n        wmapping: dict = weather_mapping.get(weather_code)\n        weather_name = wmapping.get(\"name\") if wmapping else None\n        \n        cursor.execute(\"\"\"\n            INSERT INTO factairvisualtable (\n                date_time, location_id, main_pollution_code,\n                aqi_us, temperature_c, pressure,\n                humidity, wind_speed, wind_direction, weather_text\n            )\n            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) \n            ON CONFLICT (date_time, location_id, main_pollution_code) DO NOTHING;\n        \"\"\",(\n            date_time_obj_bangkok,\n            location_id,\n            main_code,\n            contents_data[\"current\"][\"pollution\"][\"aqius\"],\n            weather_data.get(\"tp\"),\n            weather_data.get(\"pr\"),\n            weather_data.get(\"hu\"),\n            weather_data.get(\"ws\"),\n            weather_data.get(\"wd\"),\n            weather_name\n        ))\n\n        conn.commit()\n        logging.info(\"Finished load_airvisual_to_postgres\")\n\n    except Exception as e:\n        conn.rollback()\n        logging.error(f\"Error during ETL: {e}\")\n        raise\n\n    finally:\n        cursor.close()\n        conn.close()\n        logging.info(\"ETL to PostgreSQL Completed\")\n\n\n# ============ DAG ============ #\ndefault_args = {\n    'owner': 'Polakorn Anantapakorn Ming',\n    'start_date': datetime(2025, 3, 20),\n    'retries': 2,\n    'retry_delay': timedelta(minutes=3),\n}\n\nwith DAG(\n    dag_id='airvisual_pipeline_lat_long_v1',\n    schedule_interval=None,\n    default_args=default_args,\n    description='A simple data pipeline for airvisual API by lat long',\n    catchup=False,\n) as dag:\n\n    t1 = PythonOperator(\n        task_id='get_airvisual_data_hourly',\n        python_callable=get_data_from_airvisual,\n        retries= 2,\n        retry_delay=timedelta(minutes=5)\n    )\n\n    t2 = PythonOperator(\n        task_id='read_data_airvisual',\n        python_callable=read_json_data\n    )\n\n    t3 = PythonOperator(\n        task_id='load_data_airvisual_to_postgresql',\n        python_callable=load_airvisual_to_postgres\n    )\n\n    t1 >> t2 >> t3\n"
    }
  ],
  "sampled_time": "2025-08-22T20:53:23.054954Z",
  "analysis": {
    "is_valid_dag_file": true,
    "is_production_dag": true,
    "is_airflow_2": true,
    "processing_type": "batch",
    "has_streaming_operators": false,
    "has_ml_operators": false,
    "topology": {
      "pattern": "linear",
      "has_sensors": false,
      "has_branches": false,
      "has_fan_out": false,
      "has_fan_in": false,
      "estimated_max_parallel_width": 1,
      "estimated_branch_depth": 0,
      "has_cycles": false,
      "has_subdags": false,
      "has_task_groups": false
    },
    "tasks": {
      "total_count": 3,
      "operator_types": [
        "PythonOperator"
      ],
      "has_dynamic_mapping": false,
      "has_external_task_sensor": false
    },
    "description": "Air quality data pipeline that fetches data from AirVisual API, processes JSON data, and loads into PostgreSQL database with ETL operations",
    "complexity_score": 3
  }
}