{
  "spec_version": "1.0",
  "pipeline_id": "airflow-log-cleanup",
  "pipeline_name": "airflow-log-cleanup",
  "source": {
    "unique_key": null,
    "repo": "peterbull/bodhi-cast",
    "file_path": "airflow/dags/log_cleanup.py",
    "repo_url": "https://github.com/peterbull/bodhi-cast",
    "detected_airflow_version": null,
    "sampled_version": null,
    "sampled_commit": null,
    "sampled_time": null,
    "source_fingerprint_sha256": "5c296f390063bdd2bc2c42086eb474d73b91de5e3fb1cc3bec17f2adb93b651f"
  },
  "summary": {
    "purpose": "Periodically clean out task logs to prevent them from growing too large",
    "business_domain": "infrastructure_maintenance",
    "execution_model": "batch",
    "topology_pattern": "fan_out_fan_in",
    "notes": "Maintenance workflow for log cleanup with parallel execution for multiple workers"
  },
  "schedule": {
    "schedule_type": "cron",
    "schedule_expression": "@daily",
    "timezone": null,
    "start_date": null,
    "catchup_backfill": false
  },
  "control_flow": {
    "edges": [
      {
        "from": "start",
        "to": "log_cleanup_worker_num_1_dir_0"
      }
    ],
    "parallel_groups": [
      {
        "group_id": "log_cleanup_workers",
        "steps": [
          "log_cleanup_worker_num_1_dir_0"
        ]
      }
    ],
    "branch_points": [],
    "gates": []
  },
  "external_systems": [
    {
      "type": "filesystem",
      "name": "Airflow log directory",
      "identifier": "BASE_LOG_FOLDER",
      "details": [
        "Directory containing Airflow task logs"
      ],
      "auth": []
    }
  ],
  "data_artifacts": {
    "files": [
      {
        "identifier": "LOG_CLEANUP_PROCESS_LOCK_FILE",
        "description": "Lock file to prevent multiple cleanup processes from running simultaneously"
      }
    ],
    "tables": [],
    "buckets": [],
    "topics": []
  },
  "steps": [
    {
      "step_id": "start",
      "name": "start",
      "objective": "Initialize the log cleanup workflow",
      "mechanism": null,
      "inputs": [],
      "outputs": [],
      "external_system_refs": [],
      "parameters": [],
      "env_infra": {
        "env_vars": [],
        "mounts": [],
        "network": null,
        "other": []
      },
      "failure_handling": {
        "retries": null,
        "retry_delay": null,
        "timeout": null,
        "alerts": [],
        "idempotency": []
      }
    },
    {
      "step_id": "log_cleanup_worker_num_1_dir_0",
      "name": "log_cleanup_worker_num_1_dir_0",
      "objective": "Clean up old log files from the specified directory",
      "mechanism": null,
      "inputs": [
        {
          "type": "configuration",
          "identifier": "maxLogAgeInDays",
          "description": "Maximum age of logs to retain (from runtime configuration)"
        },
        {
          "type": "parameter",
          "identifier": "directory",
          "description": "Directory path to clean"
        },
        {
          "type": "parameter",
          "identifier": "sleep_time",
          "description": "Sleep time to stagger worker execution"
        }
      ],
      "outputs": [
        {
          "type": "file",
          "identifier": "LOG_CLEANUP_PROCESS_LOCK_FILE",
          "description": "Lock file created and removed during execution"
        }
      ],
      "external_system_refs": [
        {
          "type": "filesystem",
          "name": "Airflow log directory",
          "identifier": "BASE_LOG_FOLDER"
        }
      ],
      "parameters": [
        {
          "key": "directory",
          "value": "BASE_LOG_FOLDER"
        },
        {
          "key": "sleep_time",
          "value": "3"
        }
      ],
      "env_infra": {
        "env_vars": [],
        "mounts": [],
        "network": null,
        "other": []
      },
      "failure_handling": {
        "retries": 1,
        "retry_delay": "1 minute",
        "timeout": null,
        "alerts": [
          "email_on_failure"
        ],
        "idempotency": [
          "lock_file_mechanism"
        ]
      }
    }
  ],
  "quality": {
    "confidence": "medium",
    "extraction_warnings": [
      "NUMBER_OF_WORKERS is set to 1 in the code, so only one worker step is generated. The code structure suggests support for multiple workers but only one is configured.",
      "DIRECTORIES_TO_DELETE may contain multiple directories but only one is shown in the generated steps",
      "Some Airflow-specific configuration references remain in the extracted parameters"
    ]
  },
  "generation": {
    "generated_at": "2025-12-16T14:19:33.671437Z"
  },
  "variant": {
    "class_id": "C3",
    "class_name": "NO_EXECUTION_MECHANISM",
    "canonical_pipeline_id": "peterbull__bodhi-cast__log_cleanup.py",
    "spec_pipeline_id": "airflow-log-cleanup"
  }
}