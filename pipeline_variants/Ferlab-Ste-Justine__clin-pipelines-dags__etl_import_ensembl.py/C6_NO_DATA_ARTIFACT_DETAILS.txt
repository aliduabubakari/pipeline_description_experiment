Pipeline Summary:
- Pipeline ID: Ferlab-Ste-Justine__clin-pipelines-dags__etl_import_ensembl.py
- Name: etl_import_ensembl
- Purpose: ETL pipeline for importing Ensembl genomic data from FTP to object storage and processing with Spark
- Execution model: batch
- Topology pattern: linear
- Business domain: genomics
- Notes: Downloads Ensembl mapping files from FTP, checks for new versions, uploads to object storage, then processes with Spark

Control Flow:
- Dependencies (directed edges):
  - file -> table

External Systems:
- http_api: Ensembl FTP (ftp.ensembl.org)
- object_storage: S3-compatible storage (cqgc-{env}-app-datalake)
- cloud_service: Kubernetes (ETL context)

Pipeline Steps:
1. file — file
  - Objective: Download Ensembl mapping files from FTP, check for new versions, and upload to object storage
  - Mechanism: python_callable
  - Step parameters:
    - types=canonical,ena,entrez,refseq,uniprot
2. table — table
  - Objective: Process Ensembl mapping files with Spark and load into structured table
  - Mechanism: spark_job
  - Step parameters:
    - spark_class=bio.ferlab.datalake.spark3.publictables.ImportPublicTable
    - spark_config=config-etl-large
    - table_name=ensembl_mapping
    - steps=default
    - app_name=etl_import_ensembl_table

Scheduling:
- Schedule type: manual
- Start date: 2022-01-01
