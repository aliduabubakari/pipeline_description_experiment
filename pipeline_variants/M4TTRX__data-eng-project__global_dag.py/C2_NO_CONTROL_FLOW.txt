Pipeline Summary:
- Pipeline ID: M4TTRX__data-eng-project__global_dag.py
- Name: global_dag
- Purpose: ETL pipeline for processing death records and power plant data from French government APIs, with data ingestion, cleansing, and loading to PostgreSQL
- Execution model: batch
- Topology pattern: staged_etl
- Business domain: government_data
- Notes: Pipeline has two main stages: ingestion and staging, with conditional branching for death data processing

Data Artifacts / I-O Identifiers:
- file: dags/data/ingestion/death_resources.json (List of death data resources from API)
- file: dags/data/ingestion/nuclear_plants.json (Nuclear plant metadata from API)
- file: dags/data/ingestion/thermal_plants.json (Thermal plant metadata from API)
- file: dags/data/ingestion/city_geo_loc.csv (City geographic location data)
- file: dags/data/ingestion/death_*.txt (Raw death record files)
- file: dags/data/ingestion/nuclear.csv (Raw nuclear plant data)
- file: dags/data/ingestion/thermal_plants_.csv (Raw thermal plant data)
- file: dags/data/staging/thermal_plants_clean.csv (Cleaned thermal plant data)
- file: dags/data/staging/nuclear_clean_datas.csv (Cleaned nuclear plant data)
- file: dags/sql/tmp/death_insert.sql (Generated SQL queries for death data insertion)
- file: dags/sql/tmp/plant_insert.sql (Generated SQL queries for plant data insertion)
- table: deaths (Processed death records with location and dates)
- table: power_plants (Power plant data including type, location, and capacity)

External Systems:
- http_api: data.gouv.fr API (https://www.data.gouv.fr/api/1/datasets/)
- database: PostgreSQL (postgres_default)
- database: Redis (redis:6379)

Pipeline Steps:
- end_global — end_global
  - Objective: End the global pipeline
  - Mechanism: unknown
- ingestion_pipeline.end — end
  - Objective: End the ingestion stage
  - Mechanism: unknown
- ingestion_pipeline.get_city_code_geo — get_city_code_geo
  - Objective: Download city geographic location data
  - Mechanism: shell_command
  - Inputs: url:https://static.data.gouv.fr/resources/communes-de-france-base-des-codes-postaux/20200309-131459/communes-departement-region.csv
  - Outputs: file:dags/data/ingestion/city_geo_loc.csv
  - Step parameters:
    - bash_command=curl https://static.data.gouv.fr/resources/communes-de-france-base-des-codes-postaux/20200309-131459/communes-departement-region.csv --output /opt/airflow/dags/data/ingestion/city_geo_loc.csv
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- ingestion_pipeline.get_death_resource_list — get_death_resource_list
  - Objective: Fetch list of death data resources
  - Mechanism: python_callable
  - Inputs: url:https://www.data.gouv.fr/api/1/datasets/5de8f397634f4164071119c5
  - Outputs: file:dags/data/ingestion/death_resources.json
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- ingestion_pipeline.get_death_resources — get_death_resources
  - Objective: Download death data files from resource list
  - Mechanism: python_callable
  - Inputs: file:dags/data/ingestion/death_resources.json
  - Outputs: file:dags/data/ingestion/death_*.txt
  - Step parameters:
    - max_resource=5
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- ingestion_pipeline.get_nuclear_data — get_nuclear_data
  - Objective: Extract nuclear plant CSV data from metadata
  - Mechanism: python_callable
  - Inputs: file:dags/data/ingestion/nuclear_plants.json
  - Outputs: file:dags/data/ingestion/nuclear.csv
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- ingestion_pipeline.get_nuclear_json — get_nuclear_json
  - Objective: Download nuclear plant metadata
  - Mechanism: shell_command
  - Inputs: url:https://www.data.gouv.fr/api/1/datasets/63587afc1e8e90e9ce487174/
  - Outputs: file:dags/data/ingestion/nuclear_plants.json
  - Step parameters:
    - bash_command=curl https://www.data.gouv.fr/api/1/datasets/63587afc1e8e90e9ce487174/ --output /opt/airflow/dags/data/ingestion/nuclear_plants.json
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- ingestion_pipeline.get_thermal_data — get_thermal_data
  - Objective: Extract thermal plant CSV data from metadata
  - Mechanism: python_callable
  - Inputs: file:dags/data/ingestion/thermal_plants.json
  - Outputs: file:dags/data/ingestion/thermal_plants_.csv
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- ingestion_pipeline.get_thermal_json — get_thermal_plants_json
  - Objective: Download thermal plant metadata
  - Mechanism: shell_command
  - Inputs: url:https://www.data.gouv.fr/api/1/datasets/63587afb1cc488641390f68e/
  - Outputs: file:dags/data/ingestion/thermal_plants.json
  - Step parameters:
    - bash_command=curl https://www.data.gouv.fr/api/1/datasets/63587afb1cc488641390f68e/ --output /opt/airflow/dags/data/ingestion/thermal_plants.json
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- ingestion_pipeline.start — start
  - Objective: Start the ingestion stage
  - Mechanism: unknown
- staging_pipeline.clean_tmp_death_files — clean_tmp_death_files
  - Objective: Clean up temporary death data files
  - Mechanism: python_callable
  - Inputs: redis_key:death_raw, file:dags/sql/tmp/death_insert.sql
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.cleanse_death_data — cleanse_death_data
  - Objective: Process raw death data and generate SQL queries
  - Mechanism: python_callable
  - Inputs: redis_key:death_raw, file:dags/data/ingestion/city_geo_loc.csv
  - Outputs: file:dags/sql/tmp/death_insert.sql
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.create_death_table — create_death_table
  - Objective: Create deaths table in database
  - Mechanism: sql_query
  - Inputs: file:dags/sql/create_death_table.sql
  - Outputs: table:deaths
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.create_plant_persist_sql_query — create_plant_persist_sql_query
  - Objective: Generate SQL queries for plant data insertion
  - Mechanism: python_callable
  - Inputs: file:dags/data/staging/thermal_plants_clean.csv, file:dags/data/staging/nuclear_clean_datas.csv
  - Outputs: file:dags/sql/tmp/plant_insert.sql
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.create_power_plants_table — create_power_plants_table
  - Objective: Create power plants table in database
  - Mechanism: sql_query
  - Inputs: file:dags/sql/create_power_plant_table.sql
  - Outputs: table:power_plants
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.death_emptiness_check — death_emptiness_check
  - Objective: Check if death SQL file has content and branch accordingly
  - Mechanism: python_callable
  - Inputs: file:dags/sql/tmp/death_insert.sql
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.import_nuclear_clean_data — import_nuclear_clean_data
  - Objective: Clean and transform nuclear plant data
  - Mechanism: python_callable
  - Inputs: file:dags/data/ingestion/nuclear.csv
  - Outputs: file:dags/data/staging/nuclear_clean_datas.csv
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.import_thermal_clean_data — import_thermal_clean_data
  - Objective: Clean and transform thermal plant data
  - Mechanism: python_callable
  - Inputs: file:dags/data/ingestion/thermal_plants_.csv
  - Outputs: file:dags/data/staging/thermal_plants_clean.csv
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.load_data_from_ingestion — load_data_from_ingestion
  - Objective: Load death data files and track imports in Redis
  - Mechanism: python_callable
  - Inputs: file:dags/data/ingestion/death_*.txt
  - Outputs: redis_key:death_raw, redis_key:imported_death_files
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.staging_end — staging_end
  - Objective: End the staging stage
  - Mechanism: unknown
- staging_pipeline.start — start
  - Objective: Start the staging stage
  - Mechanism: unknown
- staging_pipeline.store_deaths_in_postgres — store_deaths_in_postgres
  - Objective: Insert death data into database
  - Mechanism: sql_query
  - Inputs: file:dags/sql/tmp/death_insert.sql
  - Outputs: table:deaths
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- staging_pipeline.store_plants_in_postgres — store_plants_in_postgres
  - Objective: Insert plant data into database
  - Mechanism: sql_query
  - Inputs: file:dags/sql/tmp/plant_insert.sql
  - Outputs: table:power_plants
  - Failure handling:
    - retries: 1
    - retry delay: 10 seconds
- start_global — start_global
  - Objective: Start the global pipeline
  - Mechanism: unknown

Scheduling:
- Schedule type: manual
- Catchup/backfill: False
