global_dag is a workflow with execution model batch and topology pattern staged_etl. Its purpose is: ETL pipeline for processing death records and power plant data from French government APIs, with data ingestion, cleansing, and loading to PostgreSQL.

Scheduling: schedule type is manual, catchup/backfill is False. Control flow includes dependencies such as: start_global→ingestion_pipeline; ingestion_pipeline→staging_pipeline; staging_pipeline→end_global; ingestion_pipeline.start→ingestion_pipeline.get_city_code_geo; ingestion_pipeline.start→ingestion_pipeline.get_nuclear_json; ingestion_pipeline.start→ingestion_pipeline.get_death_resource_list; ingestion_pipeline.start→ingestion_pipeline.get_thermal_json; ingestion_pipeline.get_nuclear_json→ingestion_pipeline.get_nuclear_data; ingestion_pipeline.get_death_resource_list→ingestion_pipeline.get_death_resources; ingestion_pipeline.get_thermal_json→ingestion_pipeline.get_thermal_data; ingestion_pipeline.get_nuclear_data→ingestion_pipeline.end; ingestion_pipeline.get_death_resources→ingestion_pipeline.end.

start_global performs Start the global pipeline using mechanism unknown. ingestion_pipeline.start performs Start the ingestion stage using mechanism unknown. ingestion_pipeline.get_city_code_geo performs Download city geographic location data using mechanism shell_command. ingestion_pipeline.get_nuclear_json performs Download nuclear plant metadata using mechanism shell_command. ingestion_pipeline.get_nuclear_data performs Extract nuclear plant CSV data from metadata using mechanism python_callable. ingestion_pipeline.get_death_resource_list performs Fetch list of death data resources using mechanism python_callable. ingestion_pipeline.get_death_resources performs Download death data files from resource list using mechanism python_callable. ingestion_pipeline.get_thermal_json performs Download thermal plant metadata using mechanism shell_command. ingestion_pipeline.get_thermal_data performs Extract thermal plant CSV data from metadata using mechanism python_callable. ingestion_pipeline.end performs End the ingestion stage using mechanism unknown. staging_pipeline.start performs Start the staging stage using mechanism unknown. staging_pipeline.import_nuclear_clean_data performs Clean and transform nuclear plant data using mechanism python_callable. staging_pipeline.import_thermal_clean_data performs Clean and transform thermal plant data using mechanism python_callable. staging_pipeline.create_power_plants_table performs Create power plants table in database using mechanism sql_query. staging_pipeline.create_death_table performs Create deaths table in database using mechanism sql_query. staging_pipeline.load_data_from_ingestion performs Load death data files and track imports in Redis using mechanism python_callable. staging_pipeline.cleanse_death_data performs Process raw death data and generate SQL queries using mechanism python_callable. staging_pipeline.death_emptiness_check performs Check if death SQL file has content and branch accordingly using mechanism python_callable. staging_pipeline.store_deaths_in_postgres performs Insert death data into database using mechanism sql_query. staging_pipeline.clean_tmp_death_files performs Clean up temporary death data files using mechanism python_callable. staging_pipeline.create_plant_persist_sql_query performs Generate SQL queries for plant data insertion using mechanism python_callable. staging_pipeline.store_plants_in_postgres performs Insert plant data into database using mechanism sql_query. staging_pipeline.staging_end performs End the staging stage using mechanism unknown. end_global performs End the global pipeline using mechanism unknown.
